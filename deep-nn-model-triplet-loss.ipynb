{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load library and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-74586360c9d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_new\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTripletSiameseModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTripletDistance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSelfAttentionModel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStructuredSelfAttention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m from utils.data_loader_new import (\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mload_data_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mload_word_to_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dedupe-project/pytorch-structured-self-attentive-siamese-nets/utils/data_loader_new.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# from keras.preprocessing.sequence import pad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from model.model_new import TripletSiameseModel, TripletDistance\n",
    "from model.SelfAttentionModel import StructuredSelfAttention\n",
    "from utils.data_loader_new import (\n",
    "    load_data_set,\n",
    "    load_word_to_index,\n",
    "    load_char_to_index,\n",
    "    load_triplet_orders,\n",
    "    load_padded_data,\n",
    "    load_triplet,\n",
    "    generate_embedding,\n",
    ")\n",
    "from utils.pretrained_glove_embeddings import load_glove_embeddings\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = \"/data/dac/dedupe-project/openmap/\"  # Open map dataset \n",
    "# main_path = \"/data/dac/dedupe-project/new/\"  # Lab dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(file_path, retrain=True):\n",
    "    # Load dataset\n",
    "    df = load_data_set(file_path, retrain=retrain)\n",
    "    df.fillna(\"\", inplace=True)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    # get char to index and embedding whole dataset\n",
    "    embedding_index = load_char_to_index(df, retrain=retrain)\n",
    "    embeddings = generate_embedding(embedding_index, embedding_dim=50)\n",
    "    X, X_len = load_padded_data(df, embedding_index, char_level=True, retrain=retrain)\n",
    "\n",
    "    def truncate_non_string(X, X_len):\n",
    "        # Drop rows that have length of word vector = 0\n",
    "        truncate_index = [i for i in range(0, len(X_len)) if X_len[i] <= 0]\n",
    "        X, X_len = (\n",
    "            np.delete(X, truncate_index, axis=0),\n",
    "            np.delete(X_len, truncate_index, axis=0),\n",
    "        )\n",
    "\n",
    "        return X, X_len, sorted(truncate_index, reverse=True)\n",
    "\n",
    "    X, X_len, truncate_index = truncate_non_string(X, X_len)\n",
    "    df.drop(index=truncate_index, inplace=True)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    return df, X, X_len, embeddings, embedding_index\n",
    "\n",
    "\n",
    "def to_cuda(loader, device):\n",
    "    return [load.to(device) for load in loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load character to index successfully!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee3edbae8164465aebc29041aa5ae3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Padding', max=91956, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load padded data successfully!\n"
     ]
    }
   ],
   "source": [
    "# set cuda device\n",
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
    "# torch.cuda.set_device(device)\n",
    "\n",
    "# random_augment_train.csv\n",
    "# \"new_generated_labeled_data.csv\"\n",
    "full_generated_data_path = \"random_augment_train.csv\"  # Local dataset\n",
    "open_map_data_path = \"openmap-us-train.csv\"  # Open map dataset\n",
    "# Remember to set open map in train data\n",
    "df, X, X_len, embeddings, embedding_index = prepare_data(\n",
    "    open_map_data_path, retrain=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(X_len)\n",
    "plt.savefig(\"/data/dac/dedupe-project/image/length_X.eps\", format=\"eps\", dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate DataLoader and Triplet orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aad59f6f0a4e4d0e986b89ba57c10439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='[Generate Triplet Dataset] Start thread', max=2000, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb3e785e2654d7ab827d5771d4629ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='[Generate Triplet Dataset] Join thread', max=2000, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading triplet order successfully!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "315d6e8d7efd4382af2111508a509dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Load triplets', max=1199340, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load triplet data successfully!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 700\n",
    "df_triplet_orders = load_triplet_orders(df, retrain=True)\n",
    "print(\"Loading triplet order successfully!\")\n",
    "anc_loader, pos_loader, neg_loader = load_triplet(\n",
    "    np.array(X), X_len, df_triplet_orders, batch_size=batch_size, retrain=True\n",
    ")\n",
    "print(\"Load triplet data successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train file as pair for some methods\n",
    "train_pair_arr = []\n",
    "for row in df_triplet_orders.loc[:20000, :].itertuples():\n",
    "    anchor = row[2]\n",
    "    pos = row[3]\n",
    "    neg = row[4]\n",
    "\n",
    "    pair = {}\n",
    "    pair[\"address\"] = df.iloc[anchor].content\n",
    "    pair[\"duplicated_address\"] = df.iloc[pos].content\n",
    "    pair[\"similar\"] = 1\n",
    "    train_pair_arr.append(pair)\n",
    "\n",
    "    pair = {}\n",
    "    pair[\"address\"] = df.iloc[anchor].content\n",
    "    pair[\"duplicated_address\"] = df.iloc[neg].content\n",
    "    pair[\"similar\"] = 0\n",
    "    train_pair_arr.append(pair)\n",
    "\n",
    "pd.DataFrame(train_pair_arr).to_csv(\n",
    "    \"/data/dac/dedupe-project/train_as_pair.csv\", encoding=\"utf-8\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet Siamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TripletSiameseModel(\n",
      "  (embeddings): Embedding(59, 50)\n",
      "  (gru): GRU(50, 50, batch_first=True, bidirectional=True)\n",
      "  (linear_final): Linear(in_features=100, out_features=30, bias=True)\n",
      ") TripletDistance()\n"
     ]
    }
   ],
   "source": [
    "# Self-attention triplet model\n",
    "# triplet_siamese_300d_bi_gru: hit\n",
    "# triplet_siamese_50d_bi_gru_random: outperform\n",
    "triplet_model_path = (\n",
    "    \"/data/dac/dedupe-project/new/model/triplet_siamese_50d_bi_gru_random_openmap\"\n",
    ")\n",
    "\n",
    "lr = 0.1\n",
    "margin = 0.4\n",
    "# Load model & optimizer\n",
    "model = TripletSiameseModel(\n",
    "    embeddings=embeddings, layers=1, hid_dim=50, n_classes=30\n",
    ").to(device)\n",
    "# model = StructuredSelfAttention(embeddings=embeddings, n_classes=50).to(device)\n",
    "distance = TripletDistance(margin=margin).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "print(model, distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TripletSiameseModel(\n",
       "  (embeddings): Embedding(59, 50)\n",
       "  (gru): GRU(50, 50, batch_first=True, bidirectional=True)\n",
       "  (linear_final): Linear(in_features=100, out_features=30, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model and optimizer\n",
    "checkpoint = torch.load(triplet_model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56d1434070c482cac965d00289362e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:\t0\tAverage Loss:\t2.5393\t\tPos:\t0.888\t\tNeg:\t0.0253\t\t22\t\t\t Accuracy:\t0.9938\t\tF1-score:\t0.9937\t\t\t\n",
      "Batch:\t235\tLoss:\t0.9078\t\tPos_sim:\t0.8824\t\tNeg_sim:\t0.0258\t\t\t Accuracy:\t0.9929\t\tF1-score:\t0.9928\t\t\t"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-80cdf1b92b83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m#         torch.cuda.empty_cache()  # Empty cuda cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dedupe-project/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dedupe-project/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "epochs = 2\n",
    "best_lost = None\n",
    "early_stopping_steps = 5\n",
    "\n",
    "loss_list = []\n",
    "average_list = []\n",
    "pos_sim_list = []  # Positive distance of all eposchs\n",
    "neg_sim_list = []  # Negative distance of all epochs\n",
    "model.train()\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in tqdm(range(epochs), desc=\"Epoch\", total=epochs):\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    avg_pos_sim = 0\n",
    "    avg_neg_sim = 0\n",
    "    for batch, [anc_x, pos_x, neg_x] in enumerate(\n",
    "        zip(anc_loader, pos_loader, neg_loader)\n",
    "    ):\n",
    "        # Training model per batch\n",
    "        # Send data to graphic card - Cuda\n",
    "        anc_x, pos_x, neg_x = (\n",
    "            to_cuda(anc_x, device),\n",
    "            to_cuda(pos_x, device),\n",
    "            to_cuda(neg_x, device),\n",
    "        )\n",
    "        x, pos, neg = model(anc_x, pos_x, neg_x)\n",
    "        loss, pos_sim, neg_sim = distance(x, pos, neg)\n",
    "\n",
    "        # Append to batch list\n",
    "        avg_loss += float(loss)\n",
    "        avg_pos_sim += pos_sim.mean()\n",
    "        avg_neg_sim += neg_sim.mean()\n",
    "\n",
    "        # F1 and Acc\n",
    "        y_true = np.concatenate([np.ones(len(pos_sim)), np.zeros(len(pos_sim))])\n",
    "        y_pred = np.concatenate(\n",
    "            [\n",
    "                [1 if y > 0.665 else 0 for y in pos_sim.to(\"cpu\")],\n",
    "                [1 if y > 0.665 else 0 for y in neg_sim.to(\"cpu\")],\n",
    "            ]\n",
    "        )\n",
    "        # Gradient\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #         torch.cuda.empty_cache()  # Empty cuda cache\n",
    "        print(\n",
    "            \"\\rBatch:\\t{}\\tLoss:\\t{}\\t\\tPos_sim:\\t{}\\t\\tNeg_sim:\\t{}\\t\\t\".format(\n",
    "                batch,\n",
    "                round(float(loss), 4),\n",
    "                round(float(pos_sim.mean()), 4),\n",
    "                round(float(neg_sim.mean()), 4),\n",
    "            ),\n",
    "            end=\"\",\n",
    "        )\n",
    "        print(\n",
    "            \"\\t Accuracy:\\t{}\\t\\tF1-score:\\t{}\\t\\t\".format(\n",
    "                round(accuracy_score(y_true, y_pred), 4),\n",
    "                round(f1_score(y_true, y_pred), 4),\n",
    "            ),\n",
    "            end=\"\",\n",
    "        )\n",
    "    # Average loss and distance of all epochs\n",
    "    avg_loss /= len(anc_loader)\n",
    "    avg_pos_sim /= len(anc_loader)\n",
    "    avg_neg_sim /= len(anc_loader)\n",
    "\n",
    "    loss_list.append(avg_loss)\n",
    "    pos_sim_list.append(avg_pos_sim)\n",
    "    neg_sim_list.append(avg_neg_sim)\n",
    "\n",
    "    print(\n",
    "        \"\\rEpoch:\\t{}\\tAverage Loss:\\t{}\\t\\tPos:\\t{}\\t\\tNeg:\\t{}\\t\\t\".format(\n",
    "            epoch,\n",
    "            round(avg_loss, 4),\n",
    "            round(float(avg_pos_sim), 4),\n",
    "            round(float(avg_neg_sim), 4),\n",
    "        )\n",
    "    )\n",
    "    if best_lost is None or best_lost > avg_loss:\n",
    "        best_lost = avg_loss\n",
    "        forward_index = 0\n",
    "        #         Save model\n",
    "        torch.save(\n",
    "            {\"model\": model.state_dict(), \"optimizer\": optimizer.state_dict()},\n",
    "            triplet_model_path,\n",
    "        )\n",
    "    else:\n",
    "        # Early stopping after reachs {early_stopping_steps} steps\n",
    "        forward_index += 1\n",
    "        if forward_index == early_stopping_steps or best_lost == 0:\n",
    "            break\n",
    "    if best_lost < 1:\n",
    "        break\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(\n",
    "    {\"model\": model.state_dict(), \"optimizer\": optimizer.state_dict()},\n",
    "    triplet_model_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/data/dac/dedupe-project/test/\"\n",
    "test_df = pd.read_csv(path + \"test_address_3.csv\", encoding=\"ISO-8859-1\")\n",
    "test_df.fillna(\"\", inplace=True)\n",
    "test_df.reset_index(inplace=True)\n",
    "test_df_1 = test_df.loc[:, [\"address\"]]\n",
    "test_df_1[\"content\"] = (\n",
    "    test_df_1[\"address\"]\n",
    "    .str.lower()\n",
    "    .str.replace(\"\\n\", \" \")\n",
    "    .str.replace(r\"[ ]+\", \" \", regex=True)\n",
    "    .str.replace(\"null\", \"\")\n",
    "    .str.replace(\"nan\", \"\")\n",
    ")\n",
    "test_df_2 = test_df.loc[:, [\"duplicated_address\"]]\n",
    "test_df_2[\"content\"] = (\n",
    "    test_df_2[\"duplicated_address\"]\n",
    "    .str.lower()\n",
    "    .str.replace(\"\\n\", \" \")\n",
    "    .str.replace(r\"[ ]+\", \" \", regex=True)\n",
    "    .str.replace(\"null\", \"\")\n",
    "    .str.replace(\"nan\", \"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(test_df_1, test_df_2):\n",
    "    # Make data loader\n",
    "    X1, X1_lens = load_padded_data(\n",
    "        pd.DataFrame(test_df_1), embedding_index, dump_path=None, retrain=True\n",
    "    )\n",
    "\n",
    "    X2, X2_lens = load_padded_data(\n",
    "        pd.DataFrame(test_df_2), embedding_index, dump_path=None, retrain=True\n",
    "    )\n",
    "\n",
    "    # Drop rows that have length of word vector = 0\n",
    "    truncate_index = [\n",
    "        i for i in range(0, len(X1_lens)) if (X1_lens[i] <= 0 or X2_lens[i] <= 0)\n",
    "    ]\n",
    "    X1, X1_lens = (\n",
    "        np.delete(X1, truncate_index, axis=0),\n",
    "        np.delete(X1_lens, truncate_index, axis=0),\n",
    "    )\n",
    "    X2, X2_lens = (\n",
    "        np.delete(X2, truncate_index, axis=0),\n",
    "        np.delete(X2_lens, truncate_index, axis=0),\n",
    "    )\n",
    "\n",
    "    def create_data_loader(X, batch_size=batch_size):\n",
    "        X, X_lens = np.array(X[0]), np.array(X[1])\n",
    "\n",
    "        # Create data loader\n",
    "        data = TensorDataset(\n",
    "            torch.from_numpy(X).type(torch.LongTensor), torch.ByteTensor(X_lens)\n",
    "        )\n",
    "        loader = DataLoader(data, batch_size=batch_size, drop_last=False)\n",
    "        return loader\n",
    "\n",
    "    return (\n",
    "        create_data_loader([X1, X1_lens]),\n",
    "        create_data_loader([X2, X2_lens]),\n",
    "        truncate_index,\n",
    "    )\n",
    "\n",
    "\n",
    "def create_test(n, test_df_1, test_df_2):\n",
    "    # Generate small test based on ground truth\n",
    "    test_df_1a = pd.DataFrame()\n",
    "    test_df_1b = pd.DataFrame()\n",
    "\n",
    "    for i1, i2 in shuffle(list(itertools.combinations(test_df_1.index, 2)))[:n]:\n",
    "        try:\n",
    "            test_df_1a = test_df_1a.append(test_df_1.iloc[i1, :])\n",
    "            test_df_1b = test_df_1b.append(test_df_2.iloc[i2, :])\n",
    "        except:\n",
    "            print(i1, i2)\n",
    "\n",
    "    test_df_1b = test_df_1b.append(test_df_1)\n",
    "    test_df_1a = test_df_1a.append(test_df_2)\n",
    "\n",
    "    test_df_1a.reset_index(inplace=True)\n",
    "    test_df_1b.reset_index(inplace=True)\n",
    "\n",
    "    return test_df_1a, test_df_1b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bda1f8abdb44634842c089f5e677bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Padding', max=49, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load padded data successfully!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1a4ebf031a4ab9aca185aea6143f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Padding', max=49, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load padded data successfully!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f690001f7e446f1af096983125e22e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:\t0.7551\t\tF1-score:\t0.8605\t\tPrecision:\t1.0\t\tRecall:\t0.7551\t\t0.05397677421569824\n"
     ]
    }
   ],
   "source": [
    "X1, X2, truncate = data_loader(test_df_2, test_df_1)\n",
    "test_df_1.drop(truncate, inplace=True)\n",
    "test_df_1.reset_index(inplace=True, drop=True)\n",
    "test_df_2.drop(truncate, inplace=True)\n",
    "test_df_2.reset_index(inplace=True, drop=True)\n",
    "\n",
    "pred_list = np.array([])\n",
    "y_true = np.array([])\n",
    "y_pred = np.array([])\n",
    "att1_list = []\n",
    "att2_list = []\n",
    "start_time = time.time()\n",
    "for a, b in tqdm(zip(X1, X2)):\n",
    "    # Send data to graphic card - Cuda0\n",
    "    a, b = to_cuda(a, device), to_cuda(b, device)\n",
    "    with torch.no_grad():\n",
    "        a, b = model(a, b)\n",
    "        a, b = a.cpu(), b.cpu()\n",
    "        a = a.reshape(a.shape[0], -1)\n",
    "        b = b.reshape(b.shape[0], -1)\n",
    "        #         att1 = att1.cpu()\n",
    "        #         att2 = att2.cpu()\n",
    "        dist = np.array(\n",
    "            [\n",
    "                cosine_similarity([a[i].numpy()], [b[i].numpy()])\n",
    "                for i in range(0, len(a))\n",
    "            ]\n",
    "        ).flatten()\n",
    "\n",
    "        y_true_curr = np.ones(len(dist))\n",
    "        y_true = np.concatenate([y_true, y_true_curr])\n",
    "\n",
    "        y_pred_curr = np.ones(len(dist))\n",
    "        y_pred_curr[np.where(dist < 0.665)[0]] = 0\n",
    "        y_pred = np.concatenate([y_pred, y_pred_curr])\n",
    "\n",
    "        pred_list = np.concatenate([pred_list, dist])\n",
    "#         att1_list.append(att1)\n",
    "#         att2_list.append(att2)\n",
    "\n",
    "print(\n",
    "    \"Accuracy:\\t{}\\t\\tF1-score:\\t{}\\t\\t\".format(\n",
    "        round(accuracy_score(y_true, y_pred), 4), round(f1_score(y_true, y_pred), 4)\n",
    "    ),\n",
    "    end=\"\",\n",
    ")\n",
    "print(\n",
    "    \"Precision:\\t{}\\t\\tRecall:\\t{}\\t\\t\".format(\n",
    "        round(precision_score(y_true, y_pred), 4),\n",
    "        round(recall_score(y_true, y_pred), 4),\n",
    "    ),\n",
    "    end=\"\",\n",
    ")\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(n, df1, df2):\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "    for i in range(0, 1):\n",
    "        test_df_1a, test_df_1b = create_test(n, df1, df2)\n",
    "        X1, X2, drop = data_loader(test_df_1a, test_df_1b)\n",
    "\n",
    "        pred_list = np.array([])\n",
    "        y_true = np.array([])\n",
    "        y_pred = np.array([])\n",
    "        # att1_list = []\n",
    "        # att2_list = []\n",
    "        start_time = time.time()\n",
    "\n",
    "        for a, b in tqdm(zip(X1, X2)):\n",
    "            # Send data to graphic card - Cuda0\n",
    "            a, b = to_cuda(a, device), to_cuda(b, device)\n",
    "            with torch.no_grad():\n",
    "                a, b = model(a, b)\n",
    "                a, b = a.cpu(), b.cpu()\n",
    "                a = a.reshape(a.shape[0], -1)\n",
    "                b = b.reshape(b.shape[0], -1)\n",
    "                #         att1 = att1.cpu()\n",
    "                #         att2 = att2.cpu()\n",
    "                dist = np.array(\n",
    "                    [\n",
    "                        cosine_similarity([a[i].numpy()], [b[i].numpy()])\n",
    "                        for i in range(0, len(a))\n",
    "                    ]\n",
    "                ).flatten()\n",
    "\n",
    "                y_true_curr = np.zeros(len(dist))\n",
    "                y_true = np.concatenate([y_true, y_true_curr])\n",
    "                if len(y_true) >= n:\n",
    "                    y_true[n:] = 1\n",
    "\n",
    "                y_pred_curr = np.ones(len(dist))\n",
    "                y_pred_curr[np.where(dist <= 0.72)[0]] = 0\n",
    "                y_pred = np.concatenate([y_pred, y_pred_curr])\n",
    "\n",
    "                pred_list = np.concatenate([pred_list, dist])\n",
    "        #         att1_list.append(att1)\n",
    "        #         att2_list.append(att2)\n",
    "        total_acc += accuracy_score(y_true, y_pred)\n",
    "        total_f1 += f1_score(y_true, y_pred)\n",
    "        print(\n",
    "            \"Accuracy:\\t{}\\t\\tF1-score:\\t{}\\t\\t\".format(\n",
    "                round(accuracy_score(y_true, y_pred), 4),\n",
    "                round(f1_score(y_true, y_pred), 4),\n",
    "            ),\n",
    "            end=\"\",\n",
    "        )\n",
    "        print(\n",
    "            \"Precision:\\t{}\\t\\tRecall:\\t{}\\t\\t\".format(\n",
    "                round(precision_score(y_true, y_pred), 4),\n",
    "                round(recall_score(y_true, y_pred), 4),\n",
    "            ),\n",
    "            end=\"\",\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        \"\\nAccuracy:\\t{}\\t\\tF1-score:\\t{}\\t\\t\".format(\n",
    "            round(total_acc, 4), round(total_f1, 4)\n",
    "        ),\n",
    "        end=\"\",\n",
    "    )\n",
    "    print(time.time() - start_time)\n",
    "    return test_df_1a, test_df_1b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee1d0de16fb749de8275b21463c14e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Padding', max=1225, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load padded data successfully!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181a7ffbc5ff475cb2bd7041309b084a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Padding', max=1225, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load padded data successfully!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "410a56e427fe4f4fba044b3f7368652c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:\t0.9527\t\tF1-score:\t0.5323\t\tPrecision:\t0.44\t\tRecall:\t0.6735\t\t\n",
      "Accuracy:\t0.9527\t\tF1-score:\t0.5323\t\t0.573052167892456\n"
     ]
    }
   ],
   "source": [
    "test_df_1a, test_df_1b = test(1176, test_df_1, test_df_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_new = pd.read_csv(path + \"test.csv\")\n",
    "# drop_single = [i for i in test_df_new.ID.unique() if sum(test_df_new.ID == i) <2]\n",
    "# test_df_new.set_index('ID', inplace=True)\n",
    "# test_df_new.drop(drop_single, inplace=True)\n",
    "\n",
    "# # Generate pairs\n",
    "# pairs = []\n",
    "# for i in test_df_new.index.unique():\n",
    "#     temp = test_df_new[test_df_new.index == i]\n",
    "#     # Generate a pair on the same row and append them to pairs\n",
    "#     pair = [temp.iloc[0, 1], temp.iloc[1, 1]]\n",
    "#     pairs.append(pair)\n",
    "\n",
    "# test_df_new = pd.DataFrame(pairs, columns=['address', 'duplicated_address'])\n",
    "\n",
    "test_df_new.fillna(\"\", inplace=True)\n",
    "test_df_new.reset_index(inplace=True)\n",
    "test_df_1 = test_df_new.loc[:, [\"address\"]]\n",
    "test_df_1[\"content\"] = (\n",
    "    test_df_1[\"address\"]\n",
    "    .str.lower()\n",
    "    .str.replace(\"\\n\", \" \")\n",
    "    .str.replace(r\"[ ]+\", \" \", regex=True)\n",
    "    .str.replace(\"null\", \"\")\n",
    "    .str.replace(\"nan\", \"\")\n",
    ")\n",
    "test_df_2 = test_df_new.loc[:, [\"duplicated_address\"]]\n",
    "test_df_2[\"content\"] = (\n",
    "    test_df_2[\"duplicated_address\"]\n",
    "    .str.lower()\n",
    "    .str.replace(\"\\n\", \" \")\n",
    "    .str.replace(r\"[ ]+\", \" \", regex=True)\n",
    "    .str.replace(\"null\", \"\")\n",
    "    .str.replace(\"nan\", \"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32223001d5941c38d4307152b1b51d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Padding', max=5114, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load padded data successfully!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62d3d3637a141d29c83e2b37ec487de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Padding', max=5114, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load padded data successfully!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b47e74c30c4fe9b5a1b0e831a09f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:\t0.9806\t\tF1-score:\t0.6426\t\tPrecision:\t0.546\t\tRecall:\t0.7807\t\t\n",
      "Accuracy:\t0.9806\t\tF1-score:\t0.6426\t\t"
     ]
    }
   ],
   "source": [
    "def test(n, df1, df2):\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "    for i in range(0, 1):\n",
    "        test_df_1a, test_df_1b = create_test(n, df1, df2)\n",
    "        X1, X2, drop = data_loader(test_df_1a, test_df_1b)\n",
    "\n",
    "        pred_list = np.array([])\n",
    "        y_true = np.array([])\n",
    "        y_pred = np.array([])\n",
    "        # att1_list = []\n",
    "        # att2_list = []\n",
    "        for a, b in tqdm(zip(X1, X2)):\n",
    "            # Send data to graphic card - Cuda0\n",
    "            a, b = to_cuda(a, device), to_cuda(b, device)\n",
    "            with torch.no_grad():\n",
    "                a, b = model(a, b)\n",
    "                a, b = a.cpu(), b.cpu()\n",
    "                a = a.reshape(a.shape[0], -1)\n",
    "                b = b.reshape(b.shape[0], -1)\n",
    "                #         att1 = att1.cpu()\n",
    "                #         att2 = att2.cpu()\n",
    "                dist = np.array(\n",
    "                    [\n",
    "                        cosine_similarity([a[i].numpy()], [b[i].numpy()])\n",
    "                        for i in range(0, len(a))\n",
    "                    ]\n",
    "                ).flatten()\n",
    "\n",
    "                y_true_curr = np.zeros(len(dist))\n",
    "                y_true = np.concatenate([y_true, y_true_curr])\n",
    "                if len(y_true) >= n:\n",
    "                    y_true[n:] = 1\n",
    "\n",
    "                y_pred_curr = np.ones(len(dist))\n",
    "                y_pred_curr[np.where(dist <= 0.72)[0]] = 0\n",
    "                y_pred = np.concatenate([y_pred, y_pred_curr])\n",
    "\n",
    "                pred_list = np.concatenate([pred_list, dist])\n",
    "        #         att1_list.append(att1)\n",
    "        #         att2_list.append(att2)\n",
    "        total_acc += accuracy_score(y_true, y_pred)\n",
    "        total_f1 += f1_score(y_true, y_pred)\n",
    "        print(\n",
    "            \"Accuracy:\\t{}\\t\\tF1-score:\\t{}\\t\\t\".format(\n",
    "                round(accuracy_score(y_true, y_pred), 4),\n",
    "                round(f1_score(y_true, y_pred), 4),\n",
    "            ),\n",
    "            end=\"\",\n",
    "        )\n",
    "        print(\n",
    "            \"Precision:\\t{}\\t\\tRecall:\\t{}\\t\\t\".format(\n",
    "                round(precision_score(y_true, y_pred), 4),\n",
    "                round(recall_score(y_true, y_pred), 4),\n",
    "            ),\n",
    "            end=\"\",\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        \"\\nAccuracy:\\t{}\\t\\tF1-score:\\t{}\\t\\t\".format(\n",
    "            round(total_acc, 4), round(total_f1, 4)\n",
    "        ),\n",
    "        end=\"\",\n",
    "    )\n",
    "    return test_df_1a, test_df_1b\n",
    "\n",
    "\n",
    "test_df_1a, test_df_1b = test(5000, test_df_1, test_df_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\n",
    "    \"/data/dac/dedupe-project/test/new/GT_added.csv\", encoding=\"ISO-8859-1\"\n",
    ")\n",
    "mistakes_df = pd.read_excel(\"/data/dac/dedupe-project/test/new/mistakes.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'misunderstanding', 'spelling mistake',\n",
       "       'typing error (translate)', 'system error', 'modified (append, add, …)',\n",
       "       'acronym', 'synonymous', 'nothing (update status by system)',\n",
       "       'don’t know zipcode'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistakes_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakes = [\n",
    "    \"misunderstanding\",\n",
    "    \"typing error (translate)\",\n",
    "    \"don’t know zipcode\",\n",
    "]\n",
    "mistake_dict = {\n",
    "    mistake: list(mistakes_df[mistake].dropna().index) for mistake in mistakes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.fillna(\"\", inplace=True)\n",
    "test_df.reset_index(inplace=True)\n",
    "test_df_1 = test_df.loc[:, [\"address\"]]\n",
    "test_df_1[\"content\"] = (\n",
    "    test_df_1[\"address\"]\n",
    "    .str.lower()\n",
    "    .str.replace(\"\\n\", \" \")\n",
    "    .str.replace(r\"[ ]+\", \" \", regex=True)\n",
    ")\n",
    "test_df_2 = test_df.loc[:, [\"duplicated_address\"]]\n",
    "test_df_2[\"content\"] = (\n",
    "    test_df_2[\"duplicated_address\"]\n",
    "    .str.lower()\n",
    "    .str.replace(\"\\n\", \" \")\n",
    "    .str.replace(r\"[ ]+\", \" \", regex=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26609552982b4840b83c5c6c2e960b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Padding', max=419, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load padded data successfully!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c11857ef9e4b49b15252e06cc0df0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Padding', max=419, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load padded data successfully!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7898cf1212674644aba1568e36993397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d70b93aa4a99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Send data to graphic card - Cuda0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_cuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_cuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-7faf6450a61e>\u001b[0m in \u001b[0;36mto_cuda\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_cuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mload\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-7faf6450a61e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_cuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mload\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X1, X2, truncate = data_loader(test_df_2, test_df_1)\n",
    "test_df_1.drop(truncate, inplace=True)\n",
    "# test_df_1.reset_index(inplace=True)\n",
    "test_df_2.drop(truncate, inplace=True)\n",
    "# test_df_2.reset_index(inplace=True)\n",
    "\n",
    "pred_list = np.array([])\n",
    "y_true = np.array([])\n",
    "y_pred = np.array([])\n",
    "att1_list = []\n",
    "att2_list = []\n",
    "for a, b in tqdm(zip(X1, X2)):\n",
    "    # Send data to graphic card - Cuda0\n",
    "    a, b = to_cuda(a), to_cuda(b)\n",
    "    with torch.no_grad():\n",
    "        a, b = model(a, b)\n",
    "        a, b = a.cpu(), b.cpu()\n",
    "        a = a.reshape(a.shape[0], -1)\n",
    "        b = b.reshape(b.shape[0], -1)\n",
    "        #         att1 = att1.cpu()\n",
    "        #         att2 = att2.cpu()\n",
    "        dist = np.array(\n",
    "            [\n",
    "                cosine_similarity([a[i].numpy()], [b[i].numpy()])\n",
    "                for i in range(0, len(a))\n",
    "            ]\n",
    "        ).flatten()\n",
    "\n",
    "        y_true_curr = np.ones(len(dist))\n",
    "        y_true = np.concatenate([y_true, y_true_curr])\n",
    "\n",
    "        y_pred_curr = np.ones(len(dist))\n",
    "        y_pred_curr[np.where(dist <= 0.83)[0]] = 0\n",
    "        y_pred = np.concatenate([y_pred, y_pred_curr])\n",
    "\n",
    "        pred_list = np.concatenate([pred_list, dist])\n",
    "#         att1_list.append(att1)\n",
    "#         att2_list.append(att2)\n",
    "\n",
    "print(\n",
    "    \"Accuracy:\\t{}\\t\\tF1-score:\\t{}\\t\\t\".format(\n",
    "        round(accuracy_score(y_true, y_pred), 4), round(f1_score(y_true, y_pred), 4)\n",
    "    ),\n",
    "    end=\"\",\n",
    ")\n",
    "print(\n",
    "    \"Precision:\\t{}\\t\\tRecall:\\t{}\\t\\t\".format(\n",
    "        round(precision_score(y_true, y_pred), 4),\n",
    "        round(recall_score(y_true, y_pred), 4),\n",
    "    ),\n",
    "    end=\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "misunderstanding\n",
      "Accuracy:\t0.6667\t\tF1-score:\t0.8\t\tPrecision:\t1.0\t\tRecall:\t0.6667\t\t\n",
      "typing error (translate)\n",
      "Accuracy:\t0.6\t\tF1-score:\t0.75\t\tPrecision:\t1.0\t\tRecall:\t0.6\t\t\n",
      "don’t know zipcode\n",
      "Accuracy:\t0.6087\t\tF1-score:\t0.7568\t\tPrecision:\t1.0\t\tRecall:\t0.6087\t\t\n"
     ]
    }
   ],
   "source": [
    "for key, value in mistake_dict.items():\n",
    "    y_t = y_true[value]\n",
    "    y_p = y_pred[value]\n",
    "    print(key)\n",
    "    print(\n",
    "        \"Accuracy:\\t{}\\t\\tF1-score:\\t{}\\t\\t\".format(\n",
    "            round(accuracy_score(y_t, y_p), 4), round(f1_score(y_t, y_p), 4)\n",
    "        ),\n",
    "        end=\"\",\n",
    "    )\n",
    "    print(\n",
    "        \"Precision:\\t{}\\t\\tRecall:\\t{}\\t\\t\".format(\n",
    "            round(precision_score(y_t, y_p), 4), round(recall_score(y_t, y_p), 4),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_1.to_csv(\"/data/dac/dedupe-project/test/new/GT_added.csv\", encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc-autonumbering": true,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
