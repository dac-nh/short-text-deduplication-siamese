{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Load library and dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-bedf272043ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_new\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTripletSiameseModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTripletDistance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSelfAttentionModel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStructuredSelfAttention\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m from utils.data_loader_new import (\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mload_data_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mload_word_to_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\short-text-deduplication-siamese\\utils\\data_loader_new.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ],
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error"
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from model.model_new import TripletSiameseModel, TripletDistance\n",
    "from model.SelfAttentionModel import StructuredSelfAttention\n",
    "from utils.data_loader_new import (\n",
    "    load_data_set,\n",
    "    load_word_to_index,\n",
    "    load_char_to_index,\n",
    "    load_triplet_orders,\n",
    "    load_padded_data,\n",
    "    load_triplet,\n",
    "    generate_embedding,\n",
    ")\n",
    "from utils.pretrained_glove_embeddings import load_glove_embeddings\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# main_path = \"/data/dac/dedupe-project/openmap/\"  # Open map dataset \n",
    "main_path = \"data/\"  # Lab dataset\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def prepare_data(file_path, retrain=True):\n",
    "    # Load dataset\n",
    "    df = load_data_set(file_path, retrain=retrain)\n",
    "    df.fillna(\"\", inplace=True)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    # get char to index and embedding whole dataset\n",
    "    embedding_index = load_char_to_index(df, retrain=retrain)\n",
    "    embeddings = generate_embedding(embedding_index, embedding_dim=50)\n",
    "    X, X_len = load_padded_data(df, embedding_index, char_level=True, retrain=retrain)\n",
    "\n",
    "    def truncate_non_string(X, X_len):\n",
    "        # Drop rows that have length of word vector = 0\n",
    "        truncate_index = [i for i in range(0, len(X_len)) if X_len[i] <= 0]\n",
    "        X, X_len = (\n",
    "            np.delete(X, truncate_index, axis=0),\n",
    "            np.delete(X_len, truncate_index, axis=0),\n",
    "        )\n",
    "\n",
    "        return X, X_len, sorted(truncate_index, reverse=True)\n",
    "\n",
    "    X, X_len, truncate_index = truncate_non_string(X, X_len)\n",
    "    df.drop(index=truncate_index, inplace=True)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    return df, X, X_len, embeddings, embedding_index\n",
    "\n",
    "\n",
    "def to_cuda(loader, device):\n",
    "    return [load.to(device) for load in loader]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# set cuda device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# torch.cuda.set_device(device)\n",
    "\n",
    "# random_augment_train.csv\n",
    "# \"new_generated_labeled_data.csv\"\n",
    "full_generated_data_path = \"random_augment_train.csv\"  # Local dataset\n",
    "open_map_data_path = \"openmap-us-train.csv\"  # Open map dataset\n",
    "# Remember to set open map in train data\n",
    "df, X, X_len, embeddings, embedding_index = prepare_data(\n",
    "    full_generated_data_path, retrain=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(X_len)\n",
    "plt.savefig(\"/data/dac/dedupe-project/image/length_X.eps\", format=\"eps\", dpi=1200)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate DataLoader and Triplet orders"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 700\n",
    "df_triplet_orders = load_triplet_orders(df, retrain=True)\n",
    "print(\"Loading triplet order successfully!\")\n",
    "anc_loader, pos_loader, neg_loader = load_triplet(\n",
    "    np.array(X), X_len, df_triplet_orders, batch_size=batch_size, retrain=True\n",
    ")\n",
    "print(\"Load triplet data successfully!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create train file as pair for some methods\n",
    "train_pair_arr = []\n",
    "for row in df_triplet_orders.loc[:20000, :].itertuples():\n",
    "    anchor = row[2]\n",
    "    pos = row[3]\n",
    "    neg = row[4]\n",
    "\n",
    "    pair = {}\n",
    "    pair[\"address\"] = df.iloc[anchor].content\n",
    "    pair[\"duplicated_address\"] = df.iloc[pos].content\n",
    "    pair[\"similar\"] = 1\n",
    "    train_pair_arr.append(pair)\n",
    "\n",
    "    pair = {}\n",
    "    pair[\"address\"] = df.iloc[anchor].content\n",
    "    pair[\"duplicated_address\"] = df.iloc[neg].content\n",
    "    pair[\"similar\"] = 0\n",
    "    train_pair_arr.append(pair)\n",
    "\n",
    "pd.DataFrame(train_pair_arr).to_csv(\n",
    "    \"/data/dac/dedupe-project/train_as_pair.csv\", encoding=\"utf-8\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Triplet Siamese"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Self-attention triplet model\n",
    "# triplet_siamese_300d_bi_gru: hit\n",
    "# triplet_siamese_50d_bi_gru_random: outperform\n",
    "triplet_model_path = (\n",
    "    \"/data/dac/dedupe-project/new/model/triplet_siamese_50d_bi_gru_random_openmap\"\n",
    ")\n",
    "\n",
    "lr = 0.1\n",
    "margin = 0.4\n",
    "# Load model & optimizer\n",
    "model = TripletSiameseModel(\n",
    "    embeddings=embeddings, layers=1, hid_dim=50, n_classes=30\n",
    ").to(device)\n",
    "# model = StructuredSelfAttention(embeddings=embeddings, n_classes=50).to(device)\n",
    "distance = TripletDistance(margin=margin).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "print(model, distance)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load model and optimizer\n",
    "checkpoint = torch.load(triplet_model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train model\n",
    "epochs = 2\n",
    "best_lost = None\n",
    "early_stopping_steps = 5\n",
    "\n",
    "loss_list = []\n",
    "average_list = []\n",
    "pos_sim_list = []  # Positive distance of all eposchs\n",
    "neg_sim_list = []  # Negative distance of all epochs\n",
    "model.train()\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in tqdm(range(epochs), desc=\"Epoch\", total=epochs):\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    avg_pos_sim = 0\n",
    "    avg_neg_sim = 0\n",
    "    for batch, [anc_x, pos_x, neg_x] in enumerate(\n",
    "            zip(anc_loader, pos_loader, neg_loader)\n",
    "    ):\n",
    "        # Training model per batch\n",
    "        # Send data to graphic card - Cuda\n",
    "        anc_x, pos_x, neg_x = (\n",
    "            to_cuda(anc_x, device),\n",
    "            to_cuda(pos_x, device),\n",
    "            to_cuda(neg_x, device),\n",
    "        )\n",
    "        x, pos, neg = model(anc_x, pos_x, neg_x)\n",
    "        loss, pos_sim, neg_sim = distance(x, pos, neg)\n",
    "\n",
    "        # Append to batch list\n",
    "        avg_loss += float(loss)\n",
    "        avg_pos_sim += pos_sim.mean()\n",
    "        avg_neg_sim += neg_sim.mean()\n",
    "\n",
    "        # F1 and Acc\n",
    "        y_true = np.concatenate([np.ones(len(pos_sim)), np.zeros(len(pos_sim))])\n",
    "        y_pred = np.concatenate(\n",
    "            [\n",
    "                [1 if y > 0.665 else 0 for y in pos_sim.to(\"cpu\")],\n",
    "                [1 if y > 0.665 else 0 for y in neg_sim.to(\"cpu\")],\n",
    "            ]\n",
    "        )\n",
    "        # Gradient\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #         torch.cuda.empty_cache()  # Empty cuda cache\n",
    "        print(\n",
    "            \"\\rBatch:\\t{}\\tLoss:\\t{}\\t\\tPos_sim:\\t{}\\t\\tNeg_sim:\\t{}\\t\\t\".format(\n",
    "                batch,\n",
    "                round(float(loss), 4),\n",
    "                round(float(pos_sim.mean()), 4),\n",
    "                round(float(neg_sim.mean()), 4),\n",
    "            ),\n",
    "            end=\"\",\n",
    "        )\n",
    "        print(\n",
    "            \"\\t Accuracy:\\t{}\\t\\tF1-score:\\t{}\\t\\t\".format(\n",
    "                round(accuracy_score(y_true, y_pred), 4),\n",
    "                round(f1_score(y_true, y_pred), 4),\n",
    "            ),\n",
    "            end=\"\",\n",
    "        )\n",
    "    # Average loss and distance of all epochs\n",
    "    avg_loss /= len(anc_loader)\n",
    "    avg_pos_sim /= len(anc_loader)\n",
    "    avg_neg_sim /= len(anc_loader)\n",
    "\n",
    "    loss_list.append(avg_loss)\n",
    "    pos_sim_list.append(avg_pos_sim)\n",
    "    neg_sim_list.append(avg_neg_sim)\n",
    "\n",
    "    print(\n",
    "        \"\\rEpoch:\\t{}\\tAverage Loss:\\t{}\\t\\tPos:\\t{}\\t\\tNeg:\\t{}\\t\\t\".format(\n",
    "            epoch,\n",
    "            round(avg_loss, 4),\n",
    "            round(float(avg_pos_sim), 4),\n",
    "            round(float(avg_neg_sim), 4),\n",
    "        )\n",
    "    )\n",
    "    if best_lost is None or best_lost > avg_loss:\n",
    "        best_lost = avg_loss\n",
    "        forward_index = 0\n",
    "        #         Save model\n",
    "        torch.save(\n",
    "            {\"model\": model.state_dict(), \"optimizer\": optimizer.state_dict()},\n",
    "            triplet_model_path,\n",
    "        )\n",
    "    else:\n",
    "        # Early stopping after reachs {early_stopping_steps} steps\n",
    "        forward_index += 1\n",
    "        if forward_index == early_stopping_steps or best_lost == 0:\n",
    "            break\n",
    "    if best_lost < 1:\n",
    "        break\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(\n",
    "    {\"model\": model.state_dict(), \"optimizer\": optimizer.state_dict()},\n",
    "    triplet_model_path,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path = \"/data/dac/dedupe-project/test/\"\n",
    "test_df = pd.read_csv(path + \"test_address_3.csv\", encoding=\"ISO-8859-1\")\n",
    "test_df.fillna(\"\", inplace=True)\n",
    "test_df.reset_index(inplace=True)\n",
    "test_df_1 = test_df.loc[:, [\"address\"]]\n",
    "test_df_1[\"content\"] = (\n",
    "    test_df_1[\"address\"]\n",
    "        .str.lower()\n",
    "        .str.replace(\"\\n\", \" \")\n",
    "        .str.replace(r\"[ ]+\", \" \", regex=True)\n",
    "        .str.replace(\"null\", \"\")\n",
    "        .str.replace(\"nan\", \"\")\n",
    ")\n",
    "test_df_2 = test_df.loc[:, [\"duplicated_address\"]]\n",
    "test_df_2[\"content\"] = (\n",
    "    test_df_2[\"duplicated_address\"]\n",
    "        .str.lower()\n",
    "        .str.replace(\"\\n\", \" \")\n",
    "        .str.replace(r\"[ ]+\", \" \", regex=True)\n",
    "        .str.replace(\"null\", \"\")\n",
    "        .str.replace(\"nan\", \"\")\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def data_loader(test_df_1, test_df_2):\n",
    "    # Make data loader\n",
    "    X1, X1_lens = load_padded_data(\n",
    "        pd.DataFrame(test_df_1), embedding_index, dump_path=None, retrain=True\n",
    "    )\n",
    "\n",
    "    X2, X2_lens = load_padded_data(\n",
    "        pd.DataFrame(test_df_2), embedding_index, dump_path=None, retrain=True\n",
    "    )\n",
    "\n",
    "    # Drop rows that have length of word vector = 0\n",
    "    truncate_index = [\n",
    "        i for i in range(0, len(X1_lens)) if (X1_lens[i] <= 0 or X2_lens[i] <= 0)\n",
    "    ]\n",
    "    X1, X1_lens = (\n",
    "        np.delete(X1, truncate_index, axis=0),\n",
    "        np.delete(X1_lens, truncate_index, axis=0),\n",
    "    )\n",
    "    X2, X2_lens = (\n",
    "        np.delete(X2, truncate_index, axis=0),\n",
    "        np.delete(X2_lens, truncate_index, axis=0),\n",
    "    )\n",
    "\n",
    "    def create_data_loader(X, batch_size=batch_size):\n",
    "        X, X_lens = np.array(X[0]), np.array(X[1])\n",
    "\n",
    "        # Create data loader\n",
    "        data = TensorDataset(\n",
    "            torch.from_numpy(X).type(torch.LongTensor), torch.ByteTensor(X_lens)\n",
    "        )\n",
    "        loader = DataLoader(data, batch_size=batch_size, drop_last=False)\n",
    "        return loader\n",
    "\n",
    "    return (\n",
    "        create_data_loader([X1, X1_lens]),\n",
    "        create_data_loader([X2, X2_lens]),\n",
    "        truncate_index,\n",
    "    )\n",
    "\n",
    "\n",
    "def create_test(n, test_df_1, test_df_2):\n",
    "    # Generate small test based on ground truth\n",
    "    test_df_1a = pd.DataFrame()\n",
    "    test_df_1b = pd.DataFrame()\n",
    "\n",
    "    for i1, i2 in shuffle(list(itertools.combinations(test_df_1.index, 2)))[:n]:\n",
    "        try:\n",
    "            test_df_1a = test_df_1a.append(test_df_1.iloc[i1, :])\n",
    "            test_df_1b = test_df_1b.append(test_df_2.iloc[i2, :])\n",
    "        except:\n",
    "            print(i1, i2)\n",
    "\n",
    "    test_df_1b = test_df_1b.append(test_df_1)\n",
    "    test_df_1a = test_df_1a.append(test_df_2)\n",
    "\n",
    "    test_df_1a.reset_index(inplace=True)\n",
    "    test_df_1b.reset_index(inplace=True)\n",
    "\n",
    "    return test_df_1a, test_df_1b\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## True Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X1, X2, truncate = data_loader(test_df_2, test_df_1)\n",
    "test_df_1.drop(truncate, inplace=True)\n",
    "test_df_1.reset_index(inplace=True, drop=True)\n",
    "test_df_2.drop(truncate, inplace=True)\n",
    "test_df_2.reset_index(inplace=True, drop=True)\n",
    "\n",
    "pred_list = np.array([])\n",
    "y_true = np.array([])\n",
    "y_pred = np.array([])\n",
    "att1_list = []\n",
    "att2_list = []\n",
    "start_time = time.time()\n",
    "for a, b in tqdm(zip(X1, X2)):\n",
    "    # Send data to graphic card - Cuda0\n",
    "    a, b = to_cuda(a, device), to_cuda(b, device)\n",
    "    with torch.no_grad():\n",
    "        a, b = model(a, b)\n",
    "        a, b = a.cpu(), b.cpu()\n",
    "        a = a.reshape(a.shape[0], -1)\n",
    "        b = b.reshape(b.shape[0], -1)\n",
    "        #         att1 = att1.cpu()\n",
    "        #         att2 = att2.cpu()\n",
    "        dist = np.array(\n",
    "            [\n",
    "                cosine_similarity([a[i].numpy()], [b[i].numpy()])\n",
    "                for i in range(0, len(a))\n",
    "            ]\n",
    "        ).flatten()\n",
    "\n",
    "        y_true_curr = np.ones(len(dist))\n",
    "        y_true = np.concatenate([y_true, y_true_curr])\n",
    "\n",
    "        y_pred_curr = np.ones(len(dist))\n",
    "        y_pred_curr[np.where(dist < 0.665)[0]] = 0\n",
    "        y_pred = np.concatenate([y_pred, y_pred_curr])\n",
    "\n",
    "        pred_list = np.concatenate([pred_list, dist])\n",
    "#         att1_list.append(att1)\n",
    "#         att2_list.append(att2)\n",
    "\n",
    "print(\n",
    "    \"Accuracy:\\t{}\\t\\tF1-score:\\t{}\\t\\t\".format(\n",
    "        round(accuracy_score(y_true, y_pred), 4), round(f1_score(y_true, y_pred), 4)\n",
    "    ),\n",
    "    end=\"\",\n",
    ")\n",
    "print(\n",
    "    \"Precision:\\t{}\\t\\tRecall:\\t{}\\t\\t\".format(\n",
    "        round(precision_score(y_true, y_pred), 4),\n",
    "        round(recall_score(y_true, y_pred), 4),\n",
    "    ),\n",
    "    end=\"\",\n",
    ")\n",
    "print(time.time() - start_time)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test(n, df1, df2):\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "    for i in range(0, 1):\n",
    "        test_df_1a, test_df_1b = create_test(n, df1, df2)\n",
    "        X1, X2, drop = data_loader(test_df_1a, test_df_1b)\n",
    "\n",
    "        pred_list = np.array([])\n",
    "        y_true = np.array([])\n",
    "        y_pred = np.array([])\n",
    "        # att1_list = []\n",
    "        # att2_list = []\n",
    "        start_time = time.time()\n",
    "\n",
    "        for a, b in tqdm(zip(X1, X2)):\n",
    "            # Send data to graphic card - Cuda0\n",
    "            a, b = to_cuda(a, device), to_cuda(b, device)\n",
    "            with torch.no_grad():\n",
    "                a, b = model(a, b)\n",
    "                a, b = a.cpu(), b.cpu()\n",
    "                a = a.reshape(a.shape[0], -1)\n",
    "                b = b.reshape(b.shape[0], -1)\n",
    "                #         att1 = att1.cpu()\n",
    "                #         att2 = att2.cpu()\n",
    "                dist = np.array(\n",
    "                    [\n",
    "                        cosine_similarity([a[i].numpy()], [b[i].numpy()])\n",
    "                        for i in range(0, len(a))\n",
    "                    ]\n",
    "                ).flatten()\n",
    "\n",
    "                y_true_curr = np.zeros(len(dist))\n",
    "                y_true = np.concatenate([y_true, y_true_curr])\n",
    "                if len(y_true) >= n:\n",
    "                    y_true[n:] = 1\n",
    "\n",
    "                y_pred_curr = np.ones(len(dist))\n",
    "                y_pred_curr[np.where(dist <= 0.72)[0]] = 0\n",
    "                y_pred = np.concatenate([y_pred, y_pred_curr])\n",
    "\n",
    "                pred_list = np.concatenate([pred_list, dist])\n",
    "        #         att1_list.append(att1)\n",
    "        #         att2_list.append(att2)\n",
    "        total_acc += accuracy_score(y_true, y_pred)\n",
    "        total_f1 += f1_score(y_true, y_pred)\n",
    "        print(\n",
    "            \"Accuracy:\\t{}\\t\\tF1-score:\\t{}\\t\\t\".format(\n",
    "                round(accuracy_score(y_true, y_pred), 4),\n",
    "                round(f1_score(y_true, y_pred), 4),\n",
    "            ),\n",
    "            end=\"\",\n",
    "        )\n",
    "        print(\n",
    "            \"Precision:\\t{}\\t\\tRecall:\\t{}\\t\\t\".format(\n",
    "                round(precision_score(y_true, y_pred), 4),\n",
    "                round(recall_score(y_true, y_pred), 4),\n",
    "            ),\n",
    "            end=\"\",\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        \"\\nAccuracy:\\t{}\\t\\tF1-score:\\t{}\\t\\t\".format(\n",
    "            round(total_acc, 4), round(total_f1, 4)\n",
    "        ),\n",
    "        end=\"\",\n",
    "    )\n",
    "    print(time.time() - start_time)\n",
    "    return test_df_1a, test_df_1b\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_df_1a, test_df_1b = test(1176, test_df_1, test_df_2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_df_new = pd.read_csv(path + \"test.csv\")\n",
    "# drop_single = [i for i in test_df_new.ID.unique() if sum(test_df_new.ID == i) <2]\n",
    "# test_df_new.set_index('ID', inplace=True)\n",
    "# test_df_new.drop(drop_single, inplace=True)\n",
    "\n",
    "# # Generate pairs\n",
    "# pairs = []\n",
    "# for i in test_df_new.index.unique():\n",
    "#     temp = test_df_new[test_df_new.index == i]\n",
    "#     # Generate a pair on the same row and append them to pairs\n",
    "#     pair = [temp.iloc[0, 1], temp.iloc[1, 1]]\n",
    "#     pairs.append(pair)\n",
    "\n",
    "# test_df_new = pd.DataFrame(pairs, columns=['address', 'duplicated_address'])\n",
    "\n",
    "test_df_new.fillna(\"\", inplace=True)\n",
    "test_df_new.reset_index(inplace=True)\n",
    "test_df_1 = test_df_new.loc[:, [\"address\"]]\n",
    "test_df_1[\"content\"] = (\n",
    "    test_df_1[\"address\"]\n",
    "        .str.lower()\n",
    "        .str.replace(\"\\n\", \" \")\n",
    "        .str.replace(r\"[ ]+\", \" \", regex=True)\n",
    "        .str.replace(\"null\", \"\")\n",
    "        .str.replace(\"nan\", \"\")\n",
    ")\n",
    "test_df_2 = test_df_new.loc[:, [\"duplicated_address\"]]\n",
    "test_df_2[\"content\"] = (\n",
    "    test_df_2[\"duplicated_address\"]\n",
    "        .str.lower()\n",
    "        .str.replace(\"\\n\", \" \")\n",
    "        .str.replace(r\"[ ]+\", \" \", regex=True)\n",
    "        .str.replace(\"null\", \"\")\n",
    "        .str.replace(\"nan\", \"\")\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test(n, df1, df2):\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "    for i in range(0, 1):\n",
    "        test_df_1a, test_df_1b = create_test(n, df1, df2)\n",
    "        X1, X2, drop = data_loader(test_df_1a, test_df_1b)\n",
    "\n",
    "        pred_list = np.array([])\n",
    "        y_true = np.array([])\n",
    "        y_pred = np.array([])\n",
    "        # att1_list = []\n",
    "        # att2_list = []\n",
    "        for a, b in tqdm(zip(X1, X2)):\n",
    "            # Send data to graphic card - Cuda0\n",
    "            a, b = to_cuda(a, device), to_cuda(b, device)\n",
    "            with torch.no_grad():\n",
    "                a, b = model(a, b)\n",
    "                a, b = a.cpu(), b.cpu()\n",
    "                a = a.reshape(a.shape[0], -1)\n",
    "                b = b.reshape(b.shape[0], -1)\n",
    "                #         att1 = att1.cpu()\n",
    "                #         att2 = att2.cpu()\n",
    "                dist = np.array(\n",
    "                    [\n",
    "                        cosine_similarity([a[i].numpy()], [b[i].numpy()])\n",
    "                        for i in range(0, len(a))\n",
    "                    ]\n",
    "                ).flatten()\n",
    "\n",
    "                y_true_curr = np.zeros(len(dist))\n",
    "                y_true = np.concatenate([y_true, y_true_curr])\n",
    "                if len(y_true) >= n:\n",
    "                    y_true[n:] = 1\n",
    "\n",
    "                y_pred_curr = np.ones(len(dist))\n",
    "                y_pred_curr[np.where(dist <= 0.72)[0]] = 0\n",
    "                y_pred = np.concatenate([y_pred, y_pred_curr])\n",
    "\n",
    "                pred_list = np.concatenate([pred_list, dist])\n",
    "        #         att1_list.append(att1)\n",
    "        #         att2_list.append(att2)\n",
    "        total_acc += accuracy_score(y_true, y_pred)\n",
    "        total_f1 += f1_score(y_true, y_pred)\n",
    "        print(\n",
    "            \"Accuracy:\\t{}\\t\\tF1-score:\\t{}\\t\\t\".format(\n",
    "                round(accuracy_score(y_true, y_pred), 4),\n",
    "                round(f1_score(y_true, y_pred), 4),\n",
    "            ),\n",
    "            end=\"\",\n",
    "        )\n",
    "        print(\n",
    "            \"Precision:\\t{}\\t\\tRecall:\\t{}\\t\\t\".format(\n",
    "                round(precision_score(y_true, y_pred), 4),\n",
    "                round(recall_score(y_true, y_pred), 4),\n",
    "            ),\n",
    "            end=\"\",\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        \"\\nAccuracy:\\t{}\\t\\tF1-score:\\t{}\\t\\t\".format(\n",
    "            round(total_acc, 4), round(total_f1, 4)\n",
    "        ),\n",
    "        end=\"\",\n",
    "    )\n",
    "    return test_df_1a, test_df_1b\n",
    "\n",
    "\n",
    "test_df_1a, test_df_1b = test(5000, test_df_1, test_df_2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mistake"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\n",
    "    \"/data/dac/dedupe-project/test/new/GT_added.csv\", encoding=\"ISO-8859-1\"\n",
    ")\n",
    "mistakes_df = pd.read_excel(\"/data/dac/dedupe-project/test/new/mistakes.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mistakes_df.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mistakes = [\n",
    "    \"misunderstanding\",\n",
    "    \"typing error (translate)\",\n",
    "    \"donâ€™t know zipcode\",\n",
    "]\n",
    "mistake_dict = {\n",
    "    mistake: list(mistakes_df[mistake].dropna().index) for mistake in mistakes\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_df.fillna(\"\", inplace=True)\n",
    "test_df.reset_index(inplace=True)\n",
    "test_df_1 = test_df.loc[:, [\"address\"]]\n",
    "test_df_1[\"content\"] = (\n",
    "    test_df_1[\"address\"]\n",
    "        .str.lower()\n",
    "        .str.replace(\"\\n\", \" \")\n",
    "        .str.replace(r\"[ ]+\", \" \", regex=True)\n",
    ")\n",
    "test_df_2 = test_df.loc[:, [\"duplicated_address\"]]\n",
    "test_df_2[\"content\"] = (\n",
    "    test_df_2[\"duplicated_address\"]\n",
    "        .str.lower()\n",
    "        .str.replace(\"\\n\", \" \")\n",
    "        .str.replace(r\"[ ]+\", \" \", regex=True)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X1, X2, truncate = data_loader(test_df_2, test_df_1)\n",
    "test_df_1.drop(truncate, inplace=True)\n",
    "# test_df_1.reset_index(inplace=True)\n",
    "test_df_2.drop(truncate, inplace=True)\n",
    "# test_df_2.reset_index(inplace=True)\n",
    "\n",
    "pred_list = np.array([])\n",
    "y_true = np.array([])\n",
    "y_pred = np.array([])\n",
    "att1_list = []\n",
    "att2_list = []\n",
    "for a, b in tqdm(zip(X1, X2)):\n",
    "    # Send data to graphic card - Cuda0\n",
    "    a, b = to_cuda(a), to_cuda(b)\n",
    "    with torch.no_grad():\n",
    "        a, b = model(a, b)\n",
    "        a, b = a.cpu(), b.cpu()\n",
    "        a = a.reshape(a.shape[0], -1)\n",
    "        b = b.reshape(b.shape[0], -1)\n",
    "        #         att1 = att1.cpu()\n",
    "        #         att2 = att2.cpu()\n",
    "        dist = np.array(\n",
    "            [\n",
    "                cosine_similarity([a[i].numpy()], [b[i].numpy()])\n",
    "                for i in range(0, len(a))\n",
    "            ]\n",
    "        ).flatten()\n",
    "\n",
    "        y_true_curr = np.ones(len(dist))\n",
    "        y_true = np.concatenate([y_true, y_true_curr])\n",
    "\n",
    "        y_pred_curr = np.ones(len(dist))\n",
    "        y_pred_curr[np.where(dist <= 0.83)[0]] = 0\n",
    "        y_pred = np.concatenate([y_pred, y_pred_curr])\n",
    "\n",
    "        pred_list = np.concatenate([pred_list, dist])\n",
    "#         att1_list.append(att1)\n",
    "#         att2_list.append(att2)\n",
    "\n",
    "print(\n",
    "    \"Accuracy:\\t{}\\t\\tF1-score:\\t{}\\t\\t\".format(\n",
    "        round(accuracy_score(y_true, y_pred), 4), round(f1_score(y_true, y_pred), 4)\n",
    "    ),\n",
    "    end=\"\",\n",
    ")\n",
    "print(\n",
    "    \"Precision:\\t{}\\t\\tRecall:\\t{}\\t\\t\".format(\n",
    "        round(precision_score(y_true, y_pred), 4),\n",
    "        round(recall_score(y_true, y_pred), 4),\n",
    "    ),\n",
    "    end=\"\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for key, value in mistake_dict.items():\n",
    "    y_t = y_true[value]\n",
    "    y_p = y_pred[value]\n",
    "    print(key)\n",
    "    print(\n",
    "        \"Accuracy:\\t{}\\t\\tF1-score:\\t{}\\t\\t\".format(\n",
    "            round(accuracy_score(y_t, y_p), 4), round(f1_score(y_t, y_p), 4)\n",
    "        ),\n",
    "        end=\"\",\n",
    "    )\n",
    "    print(\n",
    "        \"Precision:\\t{}\\t\\tRecall:\\t{}\\t\\t\".format(\n",
    "            round(precision_score(y_t, y_p), 4), round(recall_score(y_t, y_p), 4),\n",
    "        )\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_df_1.to_csv(\"/data/dac/dedupe-project/test/new/GT_added.csv\", encoding=\"utf-8\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc-autonumbering": true,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}