{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load library and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from model.model import TripletDistance, TripletSiameseModel\n",
    "from model.SelfAttentionModel import StructuredSelfAttention\n",
    "from utils.data_loader import *\n",
    "from utils.pretrained_glove_embeddings import load_glove_embeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# main_path = \"/data/dac/dedupe-project/openmap/\"  # Open map dataset\n",
    "main_path = \"/data/dac/dedupe-project/new/\"  # Lab dataset\n",
    "embedding_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_data(file_path, retrain=True):\n",
    "    # Load dataset\n",
    "    df = load_data_set(file_path)\n",
    "    df.fillna(\"\", inplace=True)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    # get char to index and embedding whole dataset\n",
    "    embedding_index = load_char_to_index(df)\n",
    "    embeddings = generate_embedding(embedding_index, embedding_dim=embedding_dim)\n",
    "    # Get X and X_len as matrix\n",
    "    X, X_len = load_padded_data(df, embedding_index, char_level=True)\n",
    "\n",
    "    def truncate_non_string(X, X_len):\n",
    "        # Drop rows that have length of word vector = 0\n",
    "        truncate_index = [i for i in range(0, len(X_len)) if X_len[i] <= 0]\n",
    "        X, X_len = (\n",
    "            np.delete(X, truncate_index, axis=0),\n",
    "            np.delete(X_len, truncate_index, axis=0),\n",
    "        )\n",
    "\n",
    "        return X, X_len, sorted(truncate_index, reverse=True)\n",
    "\n",
    "    X, X_len, truncate_index = truncate_non_string(X, X_len)\n",
    "    df.drop(index=truncate_index, inplace=True)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    return df, X, X_len, embeddings, embedding_index\n",
    "\n",
    "\n",
    "def to_cuda(loader, device):\n",
    "    return [load.to(device) for load in loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load character to index successfully!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa780eeaf8ed4d15bd9f1db59035af08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Padding', max=33698, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load padded data successfully!\n"
     ]
    }
   ],
   "source": [
    "# set cuda device\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "# random_augment_train.csv\n",
    "# new_generated_labeled_data.csv\n",
    "full_generated_data_path = \"new_generated_labeled_data.csv\"  # Local dataset\n",
    "open_map_data_path = \"openmap-us-train.csv\"  # Open map dataset\n",
    "# Remember to set open map in train data\n",
    "df, X, X_len, embeddings, embedding_index = prepare_data(\n",
    "    main_path + full_generated_data_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.hist(X_len)\n",
    "# plt.savefig(\"/data/dac/dedupe-project/image/length_X.eps\", format=\"eps\", dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate DataLoader and Triplet orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f8712207b4043cf95d825e10d0c53f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='[Generate Triplet Dataset] Start thread', max=137, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b227992e3a64a7a910b20b179767706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='[Generate Triplet Dataset] Join thread', max=137, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading triplet order successfully!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d0c4e79a76462598e95f59bf6b51a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Load triplets', max=3313919, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load triplet data successfully!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10000\n",
    "df_triplet_orders = load_triplet_orders(df)\n",
    "print(\"Loading triplet order successfully!\")\n",
    "anc_loader, pos_loader, neg_loader = load_triplet(\n",
    "    np.array(X), X_len, df_triplet_orders, batch_size=batch_size\n",
    ")\n",
    "print(\"Load triplet data successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Create train file as pair for some methods\n",
    "# train_pair_arr = []\n",
    "# for row in df_triplet_orders.loc[:20000, :].itertuples():\n",
    "#     anchor = row[2]\n",
    "#     pos = row[3]\n",
    "#     neg = row[4]\n",
    "\n",
    "#     pair = {}\n",
    "#     pair[\"address\"] = df.iloc[anchor].content\n",
    "#     pair[\"duplicated_address\"] = df.iloc[pos].content\n",
    "#     pair[\"similar\"] = 1\n",
    "#     train_pair_arr.append(pair)\n",
    "\n",
    "#     pair = {}\n",
    "#     pair[\"address\"] = df.iloc[anchor].content\n",
    "#     pair[\"duplicated_address\"] = df.iloc[neg].content\n",
    "#     pair[\"similar\"] = 0\n",
    "#     train_pair_arr.append(pair)\n",
    "\n",
    "# pd.DataFrame(train_pair_arr).to_csv(\n",
    "#     \"/data/dac/dedupe-project/train_as_pair.csv\", encoding=\"utf-8\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare validate dataset\n",
    "1. Load dataset\n",
    "2. Pre-processing\n",
    "3. Create pair of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/data/dac/dedupe-project/test/\"\n",
    "test_df = pd.read_csv(path + \"test_address_3.csv\", encoding=\"ISO-8859-1\")\n",
    "test_df.fillna(\"\", inplace=True)\n",
    "test_df_1 = test_df.loc[:, [\"address\"]]\n",
    "test_df_1[\"content\"] = (\n",
    "    test_df_1[\"address\"]\n",
    "    .str.lower()\n",
    "    .str.replace(\"\\n\", \" \")\n",
    "    .str.replace(r\"[ ]+\", \" \", regex=True)\n",
    "    .str.replace(\"null\", \"\")\n",
    "    .str.replace(\"nan\", \"\")\n",
    ")\n",
    "test_df_2 = test_df.loc[:, [\"duplicated_address\"]]\n",
    "test_df_2[\"content\"] = (\n",
    "    test_df_2[\"duplicated_address\"]\n",
    "    .str.lower()\n",
    "    .str.replace(\"\\n\", \" \")\n",
    "    .str.replace(r\"[ ]+\", \" \", regex=True)\n",
    "    .str.replace(\"null\", \"\")\n",
    "    .str.replace(\"nan\", \"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(test_df_1, test_df_2):\n",
    "    # Data Preparation pipeline\n",
    "    # Create Dataloader based on two dataframe have row 'content' in it\n",
    "    X1, X1_lens = load_padded_data(pd.DataFrame(test_df_1), embedding_index)\n",
    "    X2, X2_lens = load_padded_data(pd.DataFrame(test_df_2), embedding_index)\n",
    "\n",
    "    # Drop rows that have length of word vector = 0\n",
    "    truncate_index = [\n",
    "        i for i in range(0, len(X1_lens)) if (X1_lens[i] <= 0 or X2_lens[i] <= 0)\n",
    "    ]\n",
    "    X1, X1_lens = (\n",
    "        np.delete(X1, truncate_index, axis=0),\n",
    "        np.delete(X1_lens, truncate_index, axis=0),\n",
    "    )\n",
    "    X2, X2_lens = (\n",
    "        np.delete(X2, truncate_index, axis=0),\n",
    "        np.delete(X2_lens, truncate_index, axis=0),\n",
    "    )\n",
    "\n",
    "    def create_data_loader(X, batch_size=batch_size):\n",
    "        X, X_lens = np.array(X[0]), np.array(X[1])\n",
    "\n",
    "        # Create data loader\n",
    "        data = TensorDataset(\n",
    "            torch.from_numpy(X).type(torch.LongTensor), torch.ByteTensor(X_lens)\n",
    "        )\n",
    "        loader = DataLoader(data, batch_size=batch_size, drop_last=False)\n",
    "        return loader\n",
    "\n",
    "    return (\n",
    "        create_data_loader([X1, X1_lens]),\n",
    "        create_data_loader([X2, X2_lens]),\n",
    "        truncate_index,\n",
    "    )\n",
    "\n",
    "\n",
    "def create_test(n, test_df_1, test_df_2):\n",
    "    # Generate small test based on ground truth\n",
    "    test_df_1a = pd.DataFrame()\n",
    "    test_df_1b = pd.DataFrame()\n",
    "\n",
    "    for i1, i2 in shuffle(list(itertools.combinations(test_df_1.index, 2)))[:n]:\n",
    "        try:\n",
    "            test_df_1a = test_df_1a.append(test_df_1.iloc[i1, :])\n",
    "            test_df_1b = test_df_1b.append(test_df_2.iloc[i2, :])\n",
    "        except:\n",
    "            print(i1, i2)\n",
    "\n",
    "    test_df_1b = test_df_1b.append(test_df_1)\n",
    "    test_df_1a = test_df_1a.append(test_df_2)\n",
    "\n",
    "    test_df_1a.reset_index(inplace=True)\n",
    "    test_df_1b.reset_index(inplace=True)\n",
    "\n",
    "    return test_df_1a, test_df_1b\n",
    "\n",
    "\n",
    "def validate(model, X1, X2, device):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    dist_list = []\n",
    "    X1_embed, X2_embed = [], []\n",
    "    for a, b in zip(X1, X2):\n",
    "        # Send data to graphic card - Cuda0\n",
    "        a, b = to_cuda(a, device), to_cuda(b, device)\n",
    "        with torch.no_grad():\n",
    "            a, b = model(a, b)\n",
    "            a, b = a.cpu(), b.cpu()\n",
    "            a = a.reshape(a.shape[0], -1)\n",
    "            b = b.reshape(b.shape[0], -1)\n",
    "            #         att1 = att1.cpu()\n",
    "            #         att2 = att2.cpu()\n",
    "            X1_embed.append(a)\n",
    "            X2_embed.append(b)\n",
    "            dist = np.array(\n",
    "                [\n",
    "                    cosine_similarity([a[i].numpy()], [b[i].numpy()])\n",
    "                    for i in range(0, len(a))\n",
    "                ]\n",
    "            ).flatten()\n",
    "            dist_list.append(dist)\n",
    "\n",
    "            y_true_curr = np.zeros(len(dist))\n",
    "            y_true = np.concatenate([y_true, y_true_curr])\n",
    "\n",
    "            y_pred_curr = np.ones(len(dist))\n",
    "            y_pred_curr[np.where(dist < 0.74)[0]] = 0\n",
    "            y_pred = np.concatenate([y_pred, y_pred_curr])\n",
    "    y_true[1176:] = 1\n",
    "    return y_true, y_pred, dist_list, X1_embed, X2_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a02d9a5449044f47907e4ef4d95212a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Padding', max=1225, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load padded data successfully!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7f7727918d44f1b9cc3a8e4caf511ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Padding', max=1225, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load padded data successfully!\n"
     ]
    }
   ],
   "source": [
    "test_df_1a, test_df_1b = create_test(1176, test_df_1, test_df_2)\n",
    "test_X1, test_X2, test_drop = data_loader(test_df_1a, test_df_1b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet Siamese\n",
    "Build, train and validate Triplet Siamese Model\n",
    "1. Embedding\n",
    "2. Deep Neural Network (Loss: Minimize the loss by maximize the distance between positive and negative)\n",
    "3. Calculate Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TripletSiameseModel(\n",
      "  (embeddings): Embedding(59, 50)\n",
      "  (gru): GRU(50, 50, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (linear_final): Linear(in_features=100, out_features=30, bias=True)\n",
      ") TripletDistance()\n"
     ]
    }
   ],
   "source": [
    "# Self-attention triplet model\n",
    "# model = StructuredSelfAttention(embeddings=embeddings, n_classes=50).to(device)\n",
    "\n",
    "# tsm_gru_50hd_50c_1l_50e: hit\n",
    "# triplet_siamese_50d_bi_gru_random: outperform\n",
    "# siamese_triplet_1: new model\n",
    "# Test\n",
    "#tsm_gru_50hd_30c_1l_50e_random\n",
    "triplet_model_path = \"/data/dac/dedupe-project/new/model/tsm_gru_100hd_50c_1l_100e\"\n",
    "\n",
    "lr = 0.01\n",
    "margin = 0.4\n",
    "# Load model & optimizer\n",
    "model = TripletSiameseModel(\n",
    "    embedding_dim=[len(embedding_index), embedding_dim],\n",
    "    layers=1,\n",
    "    hid_dim=50,\n",
    "    n_classes=30,\n",
    "    bidirectional=True\n",
    ").to(device)\n",
    "distance = TripletDistance(margin=margin).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "print(model, distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load model and optimizer\n",
    "checkpoint = torch.load(triplet_model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67664944ceac44758856dc40613869a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=7, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t198\tLoss:\t7.1022\tAccuracy:\t0.9747\tF1-score:\t0.7156\tPrecision:\t0.65\t\tRecall:\t0.7959511"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-6bc6ee1d4f56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# Gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Validation to check the F1 and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dedupe/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dedupe/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "epochs = 7\n",
    "best_lost = None\n",
    "early_stopping_steps = 5\n",
    "\n",
    "loss_list = []\n",
    "average_list = []\n",
    "model.train()\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in tqdm(range(epochs), desc=\"Epoch\", total=epochs):\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    avg_pos_sim = 0\n",
    "    avg_neg_sim = 0\n",
    "    for batch, [anc_x, pos_x, neg_x] in enumerate(\n",
    "        zip(anc_loader, pos_loader, neg_loader)\n",
    "    ):\n",
    "        # Send data to graphic card - Cuda\n",
    "        anc_x, pos_x, neg_x = (\n",
    "            to_cuda(anc_x, device),\n",
    "            to_cuda(pos_x, device),\n",
    "            to_cuda(neg_x, device),\n",
    "        )\n",
    "        # Load model and compute the distance\n",
    "        x, pos, neg = model(anc_x, pos_x, neg_x)\n",
    "        loss, pos_sim, neg_sim = distance(x, pos, neg)\n",
    "\n",
    "        # Append to batch list\n",
    "        avg_loss += float(loss)\n",
    "        avg_pos_sim += pos_sim.mean()\n",
    "        avg_neg_sim += neg_sim.mean()\n",
    "\n",
    "        #         # F1 and Acc\n",
    "        #         y_true = np.concatenate([np.ones(len(pos_sim)), np.zeros(len(pos_sim))])\n",
    "        #         y_pred = np.concatenate(\n",
    "        #             [\n",
    "        #                 [1 if y > 0.665 else 0 for y in pos_sim.to(\"cpu\")],\n",
    "        #                 [1 if y > 0.665 else 0 for y in neg_sim.to(\"cpu\")],\n",
    "        #             ]\n",
    "        #         )\n",
    "        # Gradient\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Validation to check the F1 and accuracy\n",
    "        y_true, y_pred, _, _, _ = validate(model, test_X1, test_X2, device)\n",
    "        print(\n",
    "            \"\\rBatch:\\t{}\\tLoss:\\t{}\\tAccuracy:\\t{}\\tF1-score:\\t{}\\t\".format(\n",
    "                batch,\n",
    "                round(float(loss), 4),\n",
    "                round(accuracy_score(y_true, y_pred), 4),\n",
    "                round(f1_score(y_true, y_pred), 4),\n",
    "            ),\n",
    "            end=\"\",\n",
    "        )\n",
    "        print(\n",
    "            \"Precision:\\t{}\\t\\tRecall:\\t{}\".format(\n",
    "                round(precision_score(y_true, y_pred), 4),\n",
    "                round(recall_score(y_true, y_pred), 4),\n",
    "            ),\n",
    "            end=\"\",\n",
    "        )\n",
    "    # Average loss and distance of all epochs\n",
    "    avg_loss /= len(anc_loader)\n",
    "    avg_pos_sim /= len(anc_loader)\n",
    "    avg_neg_sim /= len(anc_loader)\n",
    "\n",
    "    loss_list.append(avg_loss)\n",
    "    print(\n",
    "        \"\\rEpoch:\\t{}\\tAverage Loss:\\t{}\\t\\tPos:\\t{}\\t\\tNeg:\\t{}\\t\\t\".format(\n",
    "            epoch,\n",
    "            round(avg_loss, 4),\n",
    "            round(float(avg_pos_sim), 4),\n",
    "            round(float(avg_neg_sim), 4),\n",
    "        ),\n",
    "        end=\"\",\n",
    "    )\n",
    "    if best_lost is None or best_lost > avg_loss:\n",
    "        best_lost = avg_loss\n",
    "        forward_index = 0\n",
    "        #Save model\n",
    "        torch.save(\n",
    "            {\"model\": model.state_dict(), \"optimizer\": optimizer.state_dict()},\n",
    "            triplet_model_path,\n",
    "        )\n",
    "    else:\n",
    "        # Early stopping after reachs {early_stopping_steps} steps\n",
    "        forward_index += 1\n",
    "        if forward_index == early_stopping_steps or best_lost == 0:\n",
    "            break\n",
    "    if best_lost < 1:\n",
    "        break\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c188cce31162484e8ecee2384059a4e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Model to classify duplicate or not\n",
    "y_true = []\n",
    "\n",
    "train_X1 = []\n",
    "train_X2 = []\n",
    "for a, b in tqdm(zip(X1, X2)):\n",
    "    # Send data to graphic card - Cuda0\n",
    "    a, b = to_cuda(a, device), to_cuda(b, device)\n",
    "    with torch.no_grad():\n",
    "        a, b = model(a, b)\n",
    "        a, b = a.cpu(), b.cpu()\n",
    "        a = a.reshape(a.shape[0], -1)\n",
    "        b = b.reshape(b.shape[0], -1)\n",
    "        # Cast to numpy and append to list\n",
    "        train_X1.append(a.data.numpy())\n",
    "        train_X2.append(b.data.numpy())\n",
    "        # Get true label of second train dataset\n",
    "        y_true_curr = np.ones(len(a))\n",
    "        y_true = np.concatenate([y_true, y_true_curr])\n",
    "\n",
    "train_X1 = np.concatenate(train_X1)\n",
    "train_X2 = np.concatenate(train_X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(\n",
    "    {\"model\": model.state_dict(), \"optimizer\": optimizer.state_dict()},\n",
    "    triplet_model_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fodor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs(df1, df2, pair):\n",
    "    df_out1 = []\n",
    "    df_out2 = []\n",
    "    y = []\n",
    "    for row in pair.itertuples():\n",
    "        # Create two test dataframe based on index file\n",
    "        # index, l_id, r_id, label\n",
    "        df_out1.append(df1[\"content\"][df1[\"id\"] == row[1]])\n",
    "        df_out2.append(df2[\"content\"][df2[\"id\"] == row[2]])\n",
    "        y.append(row[3])\n",
    "\n",
    "    df_out1 = pd.DataFrame(pd.concat(df_out1))\n",
    "    df_out2 = pd.DataFrame(pd.concat(df_out2))\n",
    "\n",
    "    # Create DataLoader\n",
    "    X1, X2, drop = data_loader(df_out1, df_out2)\n",
    "\n",
    "    return X1, X2, y, df_out1, df_out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612039d1699644beac9d9385c7c0762f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Padding', max=189, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load padded data successfully!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7938fc9d4a624f60b9b4147d8883ee26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Padding', max=189, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load padded data successfully!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639eb4fb881941fd9537864dfb27fc76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Padding', max=567, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load padded data successfully!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b332cd4301dc47c5abef427bb17b33f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Padding', max=567, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load padded data successfully!\n"
     ]
    }
   ],
   "source": [
    "# Prepare data to input as DataLoader\n",
    "fd_a = pd.read_csv(path + \"fodor/tableA.csv\")\n",
    "fd_a[\"content\"] = fd_a[\"addr\"] + fd_a[\"city\"] + fd_a[\"type\"]\n",
    "fd_a[\"content\"] = fd_a[\"content\"].str.replace(\"'\", \"\").str.replace(\"`\", \" \")\n",
    "\n",
    "fd_b = pd.read_csv(path + \"fodor/tableB.csv\")\n",
    "fd_b[\"content\"] = fd_b[\"addr\"] + fd_b[\"city\"] + fd_b[\"type\"]\n",
    "fd_b[\"content\"] = fd_b[\"content\"].str.replace(\"'\", \"\").str.replace(\"`\", \" \")\n",
    "\n",
    "fd_test_pair = pd.read_csv(path + \"fodor/test.csv\")\n",
    "fd_train_pair = pd.read_csv(path + \"fodor/train.csv\")\n",
    "\n",
    "X1_test, X2_test, y_test, df_test_1, df_test_2 = create_pairs(fd_a, fd_b, fd_test_pair)\n",
    "X1_train, X2_train, y_train, df_train_1, df_train_2 = create_pairs(\n",
    "    fd_a, fd_b, fd_train_pair\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b29d1ba60953403690fbc81dd2637b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:\t0.5079\t\tF1-score:\t0.2677\tPrecision:\t0.1619\t\tRecall:\t0.7727"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "pred_list = []\n",
    "for a, b in tqdm(zip(X1_test, X2_test)):\n",
    "    # Send data to graphic card - Cuda0\n",
    "    a, b = to_cuda(a, device), to_cuda(b, device)\n",
    "    with torch.no_grad():\n",
    "        a, b = model(a, b)\n",
    "        a, b = a.cpu(), b.cpu()\n",
    "        a = a.reshape(a.shape[0], -1)\n",
    "        b = b.reshape(b.shape[0], -1)\n",
    "        #         att1 = att1.cpu()\n",
    "        #         att2 = att2.cpu()\n",
    "        dist = np.array(\n",
    "            [\n",
    "                cosine_similarity([a[i].numpy()], [b[i].numpy()])\n",
    "                for i in range(0, len(a))\n",
    "            ]\n",
    "        ).flatten()\n",
    "        #         y_pred_curr = np.ones(len(dist))\n",
    "        #         y_pred_curr[np.where(dist < 0.35039073)[0]] = 0\n",
    "        #         y_pred = np.concatenate([y_pred, y_pred_curr])\n",
    "        y_pred_curr = clf.predict(dist.reshape(-1, 1))\n",
    "        y_pred = np.concatenate([y_pred, y_pred_curr])\n",
    "\n",
    "        pred_list = np.concatenate([pred_list, dist])\n",
    "\n",
    "print(\n",
    "    \"Accuracy:\\t{}\\t\\tF1-score:\\t{}\\t\".format(\n",
    "        round(accuracy_score(y_test, y_pred), 4), round(f1_score(y_test, y_pred), 4)\n",
    "    ),\n",
    "    end=\"\",\n",
    ")\n",
    "print(\n",
    "    \"Precision:\\t{}\\t\\tRecall:\\t{}\".format(\n",
    "        round(precision_score(y_test, y_pred), 4),\n",
    "        round(recall_score(y_test, y_pred), 4),\n",
    "    ),\n",
    "    end=\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6703 melrose ave.   los angeles californian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>1915 fillmore st.   san francisco italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>3570 las vegas blvd. s   las vegas continental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>260 e. paces ferry road atlantacontinental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>3130 piedmont road atlantaamerican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>3393 peachtree rd. atlantacontinental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>730 n. la cienega blvd.   los angeles french</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>224 ponce de leon ave. atlantasouthern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>3645 las vegas blvd. s   las vegas buffets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>249 e. 50th st.   new york french</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content\n",
       "6        6703 melrose ave.   los angeles californian\n",
       "501        1915 fillmore st.   san francisco italian\n",
       "70    3570 las vegas blvd. s   las vegas continental\n",
       "384       260 e. paces ferry road atlantacontinental\n",
       "76                3130 piedmont road atlantaamerican\n",
       "..                                               ...\n",
       "422            3393 peachtree rd. atlantacontinental\n",
       "133     730 n. la cienega blvd.   los angeles french\n",
       "86            224 ponce de leon ave. atlantasouthern\n",
       "424       3645 las vegas blvd. s   las vegas buffets\n",
       "44                 249 e. 50th st.   new york french\n",
       "\n",
       "[167 rows x 1 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_1[np.array(y_test) == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>8284 melrose ave.   los angeles   french bistro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>1870 fillmore st.   san francisco american</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>3595 las vegas blvd. s.   las vegas continental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>490 e. paces ferry rd. ne atlantacontinental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1023 abbot kinney blvd. venice  american ( ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>2355 peachtree rd. ne atlantaitalian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>903 n. la cienega blvd.   w. hollywood   fren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>600 ponce de leon ave. atlantaitalian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>3799 las vegas blvd. s.   las vegas seafood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>243 e. 58th st.   new york city italian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               content\n",
       "230   8284 melrose ave.   los angeles   french bistro \n",
       "214         1870 fillmore st.   san francisco american\n",
       "130    3595 las vegas blvd. s.   las vegas continental\n",
       "300       490 e. paces ferry rd. ne atlantacontinental\n",
       "26    1023 abbot kinney blvd. venice  american ( ne...\n",
       "..                                                 ...\n",
       "292               2355 peachtree rd. ne atlantaitalian\n",
       "229   903 n. la cienega blvd.   w. hollywood   fren...\n",
       "156              600 ponce de leon ave. atlantaitalian\n",
       "125        3799 las vegas blvd. s.   las vegas seafood\n",
       "251            243 e. 58th st.   new york city italian\n",
       "\n",
       "[167 rows x 1 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_2[np.array(y_test) == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_df_new = pd.read_csv(path + \"test.csv\")\n",
    "# drop_single = [i for i in test_df_new.ID.unique() if sum(test_df_new.ID == i) <2]\n",
    "# test_df_new.set_index('ID', inplace=True)\n",
    "# test_df_new.drop(drop_single, inplace=True)\n",
    "\n",
    "# # Generate pairs\n",
    "# pairs = []\n",
    "# for i in test_df_new.index.unique():\n",
    "#     temp = test_df_new[test_df_new.index == i]\n",
    "#     # Generate a pair on the same row and append them to pairs\n",
    "#     pair = [temp.iloc[0, 1], temp.iloc[1, 1]]\n",
    "#     pairs.append(pair)\n",
    "\n",
    "# test_df_new = pd.DataFrame(pairs, columns=['address', 'duplicated_address'])\n",
    "\n",
    "test_df_new.fillna(\"\", inplace=True)\n",
    "test_df_new.reset_index(inplace=True)\n",
    "test_df_1 = test_df_new.loc[:, [\"address\"]]\n",
    "test_df_1[\"content\"] = (\n",
    "    test_df_1[\"address\"]\n",
    "    .str.lower()\n",
    "    .str.replace(\"\\n\", \" \")\n",
    "    .str.replace(r\"[ ]+\", \" \", regex=True)\n",
    "    .str.replace(\"null\", \"\")\n",
    "    .str.replace(\"nan\", \"\")\n",
    ")\n",
    "\n",
    "test_df_2 = test_df_new.loc[:, [\"duplicated_address\"]]\n",
    "test_df_2[\"content\"] = (\n",
    "    test_df_2[\"duplicated_address\"]\n",
    "    .str.lower()\n",
    "    .str.replace(\"\\n\", \" \")\n",
    "    .str.replace(r\"[ ]+\", \" \", regex=True)\n",
    "    .str.replace(\"null\", \"\")\n",
    "    .str.replace(\"nan\", \"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe09c1fa2ee3438883a49a81b162d31e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Padding', max=5114, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load padded data successfully!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87b88639c8c46e08f35f577469ad592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Padding', max=5114, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load padded data successfully!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f01ff0f2c594389ae01483d2dbd7d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:\t0.9818\t\tF1-score:\t0.6381\t\tPrecision:\t0.5734\t\tRecall:\t0.7193\t\t\n",
      "Accuracy:\t0.9818\t\tF1-score:\t0.6381\t\t"
     ]
    }
   ],
   "source": [
    "def test(n, df1, df2):\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "    for i in range(0, 1):\n",
    "        test_df_1a, test_df_1b = create_test(n, df1, df2)\n",
    "        X1, X2, drop = data_loader(test_df_1a, test_df_1b)\n",
    "\n",
    "        pred_list = np.array([])\n",
    "        y_true = np.array([])\n",
    "        y_pred = np.array([])\n",
    "        # att1_list = []\n",
    "        # att2_list = []\n",
    "        for a, b in tqdm(zip(X1, X2)):\n",
    "            # Send data to graphic card - Cuda0\n",
    "            a, b = to_cuda(a, device), to_cuda(b, device)\n",
    "            with torch.no_grad():\n",
    "                a, b = model(a, b)\n",
    "                a, b = a.cpu(), b.cpu()\n",
    "                a = a.reshape(a.shape[0], -1)\n",
    "                b = b.reshape(b.shape[0], -1)\n",
    "                #         att1 = att1.cpu()\n",
    "                #         att2 = att2.cpu()\n",
    "                dist = np.array(\n",
    "                    [\n",
    "                        cosine_similarity([a[i].numpy()], [b[i].numpy()])\n",
    "                        for i in range(0, len(a))\n",
    "                    ]\n",
    "                ).flatten()\n",
    "\n",
    "                y_true_curr = np.zeros(len(dist))\n",
    "                y_true = np.concatenate([y_true, y_true_curr])\n",
    "                if len(y_true) >= n:\n",
    "                    y_true[n:] = 1\n",
    "\n",
    "                y_pred_curr = np.ones(len(dist))\n",
    "                y_pred_curr[np.where(dist <= 0.74)[0]] = 0\n",
    "                y_pred = np.concatenate([y_pred, y_pred_curr])\n",
    "\n",
    "                pred_list = np.concatenate([pred_list, dist])\n",
    "        #         att1_list.append(att1)\n",
    "        #         att2_list.append(att2)\n",
    "        total_acc += accuracy_score(y_true, y_pred)\n",
    "        total_f1 += f1_score(y_true, y_pred)\n",
    "        print(\n",
    "            \"Accuracy:\\t{}\\t\\tF1-score:\\t{}\\t\\t\".format(\n",
    "                round(accuracy_score(y_true, y_pred), 4),\n",
    "                round(f1_score(y_true, y_pred), 4),\n",
    "            ),\n",
    "            end=\"\",\n",
    "        )\n",
    "        print(\n",
    "            \"Precision:\\t{}\\t\\tRecall:\\t{}\\t\\t\".format(\n",
    "                round(precision_score(y_true, y_pred), 4),\n",
    "                round(recall_score(y_true, y_pred), 4),\n",
    "            ),\n",
    "            end=\"\",\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        \"\\nAccuracy:\\t{}\\t\\tF1-score:\\t{}\\t\\t\".format(\n",
    "            round(total_acc, 4), round(total_f1, 4)\n",
    "        ),\n",
    "        end=\"\",\n",
    "    )\n",
    "    return test_df_1a, test_df_1b\n",
    "\n",
    "\n",
    "test_df_1a, test_df_1b = test(5000, test_df_1, test_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\Accuracy:\t0.9641\tF1-score:\t0.6207\tPrecision:\t0.5373\t\tRecall:\t0.7347"
     ]
    }
   ],
   "source": [
    "y_true, y_pred, _, _, _ = validate(model, test_X1, test_X2, device)\n",
    "print(\n",
    "    \"\\Accuracy:\\t{}\\tF1-score:\\t{}\\t\".format(\n",
    "        round(accuracy_score(y_true, y_pred), 4), round(f1_score(y_true, y_pred), 4),\n",
    "    ),\n",
    "    end=\"\",\n",
    ")\n",
    "print(\n",
    "    \"Precision:\\t{}\\t\\tRecall:\\t{}\".format(\n",
    "        round(precision_score(y_true, y_pred), 4),\n",
    "        round(recall_score(y_true, y_pred), 4),\n",
    "    ),\n",
    "    end=\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\n",
    "    \"/data/dac/dedupe-project/test/new/GT_added.csv\", encoding=\"ISO-8859-1\"\n",
    ")\n",
    "mistakes_df = pd.read_excel(\"/data/dac/dedupe-project/test/new/mistakes.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mistakes = [\n",
    "    \"misunderstanding\",\n",
    "    \"typing error (translate)\",\n",
    "    \"don’t know zipcode\",\n",
    "]\n",
    "mistake_dict = {\n",
    "    mistake: list(mistakes_df[mistake].dropna().index) for mistake in mistakes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_df.fillna(\"\", inplace=True)\n",
    "test_df.reset_index(inplace=True)\n",
    "test_df_1 = test_df.loc[:, [\"address\"]]\n",
    "test_df_1[\"content\"] = (\n",
    "    test_df_1[\"address\"]\n",
    "    .str.lower()\n",
    "    .str.replace(\"\\n\", \" \")\n",
    "    .str.replace(r\"[ ]+\", \" \", regex=True)\n",
    ")\n",
    "test_df_2 = test_df.loc[:, [\"duplicated_address\"]]\n",
    "test_df_2[\"content\"] = (\n",
    "    test_df_2[\"duplicated_address\"]\n",
    "    .str.lower()\n",
    "    .str.replace(\"\\n\", \" \")\n",
    "    .str.replace(r\"[ ]+\", \" \", regex=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X1, X2, truncate = data_loader(test_df_2, test_df_1)\n",
    "test_df_1.drop(truncate, inplace=True)\n",
    "# test_df_1.reset_index(inplace=True)\n",
    "test_df_2.drop(truncate, inplace=True)\n",
    "# test_df_2.reset_index(inplace=True)\n",
    "\n",
    "pred_list = np.array([])\n",
    "y_true = np.array([])\n",
    "y_pred = np.array([])\n",
    "att1_list = []\n",
    "att2_list = []\n",
    "for a, b in tqdm(zip(X1, X2)):\n",
    "    # Send data to graphic card - Cuda0\n",
    "    a, b = to_cuda(a), to_cuda(b)\n",
    "    with torch.no_grad():\n",
    "        a, b = model(a, b)\n",
    "        a, b = a.cpu(), b.cpu()\n",
    "        a = a.reshape(a.shape[0], -1)\n",
    "        b = b.reshape(b.shape[0], -1)\n",
    "        #         att1 = att1.cpu()\n",
    "        #         att2 = att2.cpu()\n",
    "        dist = np.array(\n",
    "            [\n",
    "                cosine_similarity([a[i].numpy()], [b[i].numpy()])\n",
    "                for i in range(0, len(a))\n",
    "            ]\n",
    "        ).flatten()\n",
    "\n",
    "        y_true_curr = np.ones(len(dist))\n",
    "        y_true = np.concatenate([y_true, y_true_curr])\n",
    "\n",
    "        y_pred_curr = np.ones(len(dist))\n",
    "        y_pred_curr[np.where(dist <= 0.83)[0]] = 0\n",
    "        y_pred = np.concatenate([y_pred, y_pred_curr])\n",
    "\n",
    "        pred_list = np.concatenate([pred_list, dist])\n",
    "#         att1_list.append(att1)\n",
    "#         att2_list.append(att2)\n",
    "\n",
    "print(\n",
    "    \"Accuracy:\\t{}\\t\\tF1-score:\\t{}\\t\\t\".format(\n",
    "        round(accuracy_score(y_true, y_pred), 4), round(f1_score(y_true, y_pred), 4)\n",
    "    ),\n",
    "    end=\"\",\n",
    ")\n",
    "print(\n",
    "    \"Precision:\\t{}\\t\\tRecall:\\t{}\\t\\t\".format(\n",
    "        round(precision_score(y_true, y_pred), 4),\n",
    "        round(recall_score(y_true, y_pred), 4),\n",
    "    ),\n",
    "    end=\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for key, value in mistake_dict.items():\n",
    "    y_t = y_true[value]\n",
    "    y_p = y_pred[value]\n",
    "    print(key)\n",
    "    print(\n",
    "        \"Accuracy:\\t{}\\t\\tF1-score:\\t{}\\t\\t\".format(\n",
    "            round(accuracy_score(y_t, y_p), 4), round(f1_score(y_t, y_p), 4)\n",
    "        ),\n",
    "        end=\"\",\n",
    "    )\n",
    "    print(\n",
    "        \"Precision:\\t{}\\t\\tRecall:\\t{}\\t\\t\".format(\n",
    "            round(precision_score(y_t, y_p), 4), round(recall_score(y_t, y_p), 4),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_df_1.to_csv(\"/data/dac/dedupe-project/test/new/GT_added.csv\", encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
