{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Random augmentation\n",
    "import nlpaug.augmenter.char as nac\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# labeled_data_dedupe_fixed\n",
    "# labeled_data_dedupe_fixed_2\n",
    "def load_data_fill_na(path=\"data/labeled_data_dedupe__address_fixed_2.xlsx\"):\n",
    "    \"\"\"\n",
    "    Load dataset\n",
    "    :param path: path to excel file\n",
    "    :return: dataframe\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(path)\n",
    "    if df[\"similar\"].isna().any():\n",
    "        df[\"similar\"][df[\"similar\"].isna()] = 1\n",
    "    df.to_excel(path, index=False)\n",
    "    df.dropna(subset=[\"address\"], inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_classes(df, cols=\"similar\"):\n",
    "    \"\"\"\n",
    "    Count number of full triplets, anchor with postive only and anchor with negative only\n",
    "    :param df: dataframe\n",
    "    :param cols: column need to compare\n",
    "    :return: postive, negative and full triplet\n",
    "    \"\"\"\n",
    "    cid_list = df[\"cid\"].unique()\n",
    "    pos_set = set()\n",
    "    neg_set = set()\n",
    "    full_set = set()\n",
    "    for cid in cid_list:\n",
    "        if {1, 0.0} == set(df[df[\"cid\"] == cid][\"similar\"]):\n",
    "            full_set.add(cid)\n",
    "        elif {1.0} == set(df[df[\"cid\"] == cid][\"similar\"]):\n",
    "            pos_set.add(cid)\n",
    "        elif {0.0} == set(df[df[\"cid\"] == cid][\"similar\"]):\n",
    "            neg_set.add(cid)\n",
    "\n",
    "    print(\"{} Full Positive and Negative\".format(len(full_set)))\n",
    "    print(\"{} Positive only\".format(len(pos_set)))\n",
    "    print(\"{} Negative only\".format(len(neg_set)))\n",
    "\n",
    "    return pos_set, neg_set, full_set\n",
    "\n",
    "\n",
    "def reset_cis(dataframe):\n",
    "    \"\"\"\n",
    "    Reset cid to type int if necessary\n",
    "    :param dataframe: \n",
    "    :return: df: dataframe\n",
    "    \"\"\"\n",
    "    # Reset cid of dataset\n",
    "    df = dataframe.copy()\n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[\"cid\"] = le.fit_transform(df[\"cid\"].astype(str))\n",
    "\n",
    "    return df[\"cid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "26 Full Positive and Negative\n104 Positive only\n7 Negative only\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "df = load_data_fill_na()\n",
    "if df.dtypes[\"cid\"] != \"int64\":\n",
    "    df[\"cid\"] = reset_cis(df)\n",
    "\n",
    "df = df.loc[:, [\"cid\", \"address\", \"similar\"]]\n",
    "# get list positive, negative and full set\n",
    "pos_set, neg_set, full_set = get_classes(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Have done appending 4490 records for 104 positive set\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\"\"\"\n",
    "(DO NOT RUN IF NOT NECESSARY)\n",
    "Random pair a partial of dataset to an only POS class \n",
    "to create negative samples to make all full sets\n",
    "\"\"\"\n",
    "df_gen = pd.DataFrame()\n",
    "for cid in df['cid'].unique():\n",
    "    # sample 35% of all labeled dataset\n",
    "    df_sample = df.sample(frac=0.1)\n",
    "    # choose rows that have different cid from main one\n",
    "    df_sample = df_sample[~df_sample[\"cid\"].isin([cid])]\n",
    "    df_sample[\"cid\"] = cid\n",
    "    df_sample[\"similar\"] = 0\n",
    "    df_gen = df_gen.append(df_sample)\n",
    "# Append to main dataset\n",
    "df = pd.concat([df, df_gen]).sort_values(\"cid\")\n",
    "df[\"address\"] = (\n",
    "    df[\"address\"]\n",
    "    .str.lower()\n",
    "    .str.replace(\"\\n\", \" \")\n",
    "    .str.replace(r\"[ ]+\", \" \", regex=True)\n",
    "    .str.replace(\"null\", \"\")\n",
    "    .str.replace(\"nan\", \"\")\n",
    ")\n",
    "print(\n",
    "    \"Have done appending {} records \"\n",
    "    \"for {} positive set\".format(len(df_gen), len(pos_set))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "new_rows = []\n",
    "for row in df.itertuples():\n",
    "    # Augment data base on keyboard mistakes and deletion\n",
    "    new_row = list(row)[1:]\n",
    "    aug = nac.KeyboardAug()\n",
    "    augmented_texts = aug.augment(new_row[1], n=3)\n",
    "    for text in augmented_texts:\n",
    "        new_row[1] = text\n",
    "        new_rows.append(copy.copy(new_row))\n",
    "    \n",
    "    new_row = list(row)[1:]\n",
    "    aug = nac.RandomCharAug(action=\"delete\")\n",
    "    augmented_texts = aug.augment(new_row[1], n=3)\n",
    "    for text in augmented_texts:\n",
    "        new_row[1] = text\n",
    "        new_rows.append(copy.copy(new_row))\n",
    "        \n",
    "# Save file random augmentation   \n",
    "test_df = pd.DataFrame(new_rows, columns=['cid', 'address', 'similar'])\n",
    "test_df.to_csv(\"data/random_augment_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Differences\n",
    "In this segment, I analyze and collect all differences through all data. And I use those statistical problems to generate new records based on original record. I also put some probability in generating each problems to reduce the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "synonym_dict = {\n",
    "    \"name\": [\n",
    "        [\"[^a-zA-Z]inc.\", \"[^a-zA-Z]inc\"],\n",
    "        [\"[^a-zA-Z]co.\", \"[^a-zA-Z]co[^a-zA-Z]\", \"company\"],\n",
    "        [\"ltd.\", \"ltd\", \"limited\"],\n",
    "        [\"pvt.\", \"[^a-zA-Z]pvt\", \"private\"],\n",
    "        [\"[^a-zA-Z]llc.\", \"[^a-zA-Z]llc\"],\n",
    "        [\"[^a-zA-Z]no.\", \"[^a-zA-Z]no\"],\n",
    "        [\"[^a-zA-Z]us[^a-zA-Z]\", \"[^a-zA-Z]usa[^a-zA-Z]\", \"united states\"],\n",
    "        [\"[^0-9]3[^0-9]\", \"[^a-zA-Z0-9]iii[^a-zA-Z0-9]\", \"[^0-9]03[^0-9]\"],\n",
    "        [\"[^0-9]2[^0-9]\", \"[^a-zA-Z0-9]ii[^a-zA-Z0-9]\", \"[^0-9]02[^0-9]\"],\n",
    "        [\"[^0-9]1[^0-9]\", \"[^a-zA-Z0-9]i[^a-zA-Z0-9]\", \"[^0-9]01[^0-9]\"],\n",
    "    ],\n",
    "    \"address\": [\n",
    "        [\"[^0-9]1[^0-9]\", \"[^0-9]01[^0-9]\"],\n",
    "        [\"[^0-9]2[^0-9]\", \"[^0-9]02[^0-9]\"],\n",
    "        [\"[^0-9]3[^0-9]\", \"[^0-9]03[^0-9]\"],\n",
    "        [\"[^0-9]4[^0-9]\", \"[^0-9]04[^0-9]\"],\n",
    "        [\"[^0-9]5[^0-9]\", \"[^0-9]05[^0-9]\"],\n",
    "        [\"[^0-9]7[^0-9]\", \"[^0-9]06[^0-9]\"],\n",
    "        [\"[^0-9]8[^0-9]\", \"[^0-9]08[^0-9]\"],\n",
    "        [\"[^0-9]9[^0-9]\", \"[^0-9]09[^0-9]\"],\n",
    "        [\"1st\", \"first\"],\n",
    "        [\"2nd\", \"second\"],\n",
    "        [\"3rd\", \"third\"],\n",
    "        ['[^a-zA-Z]k.[^a-zA-Z]', '[^a-zA-Z]kat.[^a-zA-Z]', '[^a-zA-Z]k:[^a-zA-Z]', '[^a-zA-Z]kat:[^a-zA-Z]', '[^a-zA-Z]k[^a-zA-Z]', '[^a-zA-Z]kat[^a-zA-Z]'],\n",
    "        [\"[^a-zA-Z]area[^a-zA-Z]\", \"[^a-zA-Z]zone[^a-zA-Z]\"],\n",
    "        [\"[^a-zA-Z]no-\", \"[^a-zA-Z]no.\",\"[^a-zA-Z]no:\", \"[^a-zA-Z]no[^a-zA-Z.-]\"],\n",
    "        [\"country\", \"county\"],\n",
    "        [\"road\", \"rd[.]{1}\", \"[^a-zA-Z0-9]rd[^a-zA-Z.]\"],\n",
    "        [\n",
    "            \"street\",\n",
    "            \"[^a-zA-Z0-9]str[.]{1}\",\n",
    "            \"[^a-zA-Z0-9]str[^a-zA-Z.]\",\n",
    "            \"[^a-zA-Z0-9]st[.]{1}\",\n",
    "            \"[^a-zA-Z0-9]st[^a-zA-Z.]\",\n",
    "        ],\n",
    "        [\"drive\", \"[^a-zA-Z]dr[^a-zA-Z.]\", \"[^a-zA-Z]dr[.]{1}\"],\n",
    "        [\"avenue\", \"[^a-zA-Z]ave[.]{1}\", \"[^a-zA-Z]ave[^a-zA-Z.]\"],\n",
    "        [\"boulevard\", \"[^a-zA-Z]blvd[.]{1}\", \"blvd[^.]\"],\n",
    "        [\"lane\", \"[^a-zA-Z]ln[.]{1}\", \"[^a-zA-Z0-9]ln[^a-zA-Z.]\"],\n",
    "        [\"sector[^-]\", \"sector-\"],\n",
    "        [\"[^a-zA-Z]court[^a-zA-Z]\", \"[^a-zA-Z]ct[^a-zA-Z.]\"],\n",
    "        [\"china\", \"[^a-zA-Z]cn[^a-zA-Z]\", \"c[.]{1}n\"],\n",
    "        [\"united states\", \"u[.]{1}s\", \"[^a-zA-Z]us[^a-zA-Z]\", \"usa\"],\n",
    "        [\"vietnam\", \"viet nam\", \"[^a-zA-Z]vn[^a-zA-Z]\"],\n",
    "    ],\n",
    "}\n",
    "\n",
    "# ---- Generate new rows - start\n",
    "def generate_row(target, synonym_dict, syn_type):\n",
    "    \"\"\"\n",
    "    Generate new row by using synonym and acronym\n",
    "    :param target: content\n",
    "    :param synonym_dict: dictionary of synonym and acronym\n",
    "    :param syn_type: type of synonym and acronym\n",
    "    :return: {'code': 1 for successfull and 0 for failed, {content}: new row if code = 1\n",
    "    \"\"\"\n",
    "    result = {\"code\": 0}\n",
    "    try:\n",
    "        target_lowered = target.lower()  # get lowered target\n",
    "    except:\n",
    "        # out if target_lowered is Nan\n",
    "        return result\n",
    "    # each cluster of synonym (name)\n",
    "    for i in range(len(synonym_dict[syn_type])):\n",
    "        synonym = synonym_dict[syn_type][i]\n",
    "        # iterrate words in a synonyms cluster\n",
    "        for j in range(len(synonym)):\n",
    "            syn = synonym[j]\n",
    "            target_lowered = target.lower()  # get lowered target\n",
    "            match = re.search(\"{}\".format(syn), target_lowered)\n",
    "            if match is None:\n",
    "                continue\n",
    "\n",
    "            # Check whether replace or not\n",
    "            if match is not None:\n",
    "                syn_len = len(syn)  # Get length of replaceable word\n",
    "                target_sta = match.start()\n",
    "                target_end = target_sta + syn_len\n",
    "                if target_end < len(target) and (\n",
    "                    target[target_end].isdigit() or target[target_end].isalpha()\n",
    "                ):\n",
    "                    # if word neither is the last word of sentence nor a sub of a word, a number\n",
    "                    continue\n",
    "\n",
    "                # gamble or not base (75%)\n",
    "                if np.random.randint(0, 4, 1) == 0:\n",
    "                    break\n",
    "\n",
    "                # position of synonym's replacement\n",
    "                syn_rep_pos = j\n",
    "                while syn_rep_pos == j:\n",
    "                    syn_rep_pos = np.random.randint(0, len(synonym), 1)[0]\n",
    "                syn_rep = synonym[syn_rep_pos]\n",
    "\n",
    "                # Generate new target\n",
    "                target = target[:target_sta] + \" \" + syn_rep + \" \" + target[target_end:]\n",
    "                target = target.lower()\n",
    "                # Remove all regex special characters\n",
    "                target = re.sub(\"\\[\\.\\]\", \".\", target)\n",
    "                target = re.sub(\"\\[([a\\-z.\\^09]*)\\]\", \"\", target)\n",
    "                target = re.sub(\"\\{[0-9]+\\}\", \"\", target)\n",
    "                break\n",
    "    result = {\"code\": 1, \"content\": target}\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_new_rows(df, synonym_dict, rep_column=[\"name\", \"address\"], low=3, high=5):\n",
    "    \"\"\"\n",
    "    Generate rows by using synonym and acronym with columns name\n",
    "    :param df: dataframe\n",
    "    :param synonym_dict: dictionary of synonym and acronym\n",
    "    :param rep_column: column that use want to duplicate\n",
    "    :param low: min duplications for each row\n",
    "    :param high: max duplications for each row\n",
    "    :return: new dataframe\n",
    "    \"\"\"\n",
    "    new_rows = []\n",
    "    for row in df.itertuples():\n",
    "        # Random duplicating a rows from low times to high times\n",
    "        for i in range(np.random.randint(low, high, 1)[0]):\n",
    "            new_row = list(row)[1:]  # 1: to drop index in dataframe\n",
    "            # Generate new row for selected column(s)\n",
    "            for column in rep_column:\n",
    "                result = generate_row(\n",
    "                    row[1], synonym_dict, syn_type=column\n",
    "                )\n",
    "                if result[\"code\"] == 1:\n",
    "                    new_row[1] = result[\"content\"]\n",
    "            # Collect all new rows and store them into new_rows\n",
    "            new_rows.append(new_row)\n",
    "    df_result = pd.DataFrame(pd.DataFrame(new_rows, columns=df.columns))\n",
    "    return df_result\n",
    "# ---- Generate new rows - end\n",
    "\n",
    "\n",
    "# ---- Full pipeline for pre-processing and generate rows\n",
    "def pre_processing_pipeline(df):\n",
    "    temp_df = df.copy()\n",
    "    address_list = df[\"address\"].values.copy()\n",
    "    # All the cases are handled with a certain probability to prevent bias\n",
    "    for index, target in tqdm(enumerate(address_list)):\n",
    "        # ---- System Error - missing some thing\n",
    "        random = np.random.randint(0, 100, 1)\n",
    "        target_as_list = target.split(\",\")  # Split sentence as a list\n",
    "        if random < 16 and len(target_as_list) > 2:\n",
    "            pos_to_drop = list(set(np.random.randint(2, len(target_as_list), 3)))\n",
    "            for pos in sorted(pos_to_drop, reverse=True):\n",
    "                # Drop position that has been chosen\n",
    "                target_as_list.pop(pos)\n",
    "            target = \",\".join(target_as_list)\n",
    "\n",
    "        # ---- Zip-code case ~ 13%\n",
    "        # Zipcode error\n",
    "        zipcode = [\"000000\", \"111111\", \"123456\", \"0\", \"1\"]\n",
    "        random = np.random.randint(0, 100, 1)\n",
    "        if random <= 13:\n",
    "            match = re.search(\"[0-9]{3,6}[\\.]*$\", target)\n",
    "            if match is not None:\n",
    "                match = match.start()\n",
    "                target = (\n",
    "                    target[:match] + zipcode[np.random.randint(0, len(zipcode), 1)[0]]\n",
    "                )\n",
    "\n",
    "        # ---- Misunderstanding\n",
    "        random = np.random.randint(0, 100, 1)\n",
    "        if random < 27:\n",
    "            mis_understanding = [\n",
    "                [\n",
    "                    [\"avenue\", \"[^a-zA-Z]ave[.]{1}\", \"[^a-zA-Z]ave[^a-zA-Z.]\"],\n",
    "                    [\"boulevard\", \"[^a-zA-Z]blvd[.]{1}\", \"blvd[^.]\"],\n",
    "                    [\"drive\", \"[^a-zA-Z]dr[^a-zA-Z.]\", \"[^a-zA-Z]dr[.]{1}\"],\n",
    "                ],\n",
    "                [\n",
    "                    [\"lane\", \"[^a-zA-Z]ln[.]{1}\", \"[^a-zA-Z0-9]ln[^a-zA-Z.]\"],\n",
    "                    [\"road\", \"rd[.]{1}\", \"[^a-zA-Z0-9]rd[^a-zA-Z.]\"],\n",
    "                ],\n",
    "                [[\"[^a-zA-Z]zone[^a-zA-Z]\"], [\"[^a-zA-Z]area[^a-zA-Z]\"]],\n",
    "                [\n",
    "                    [\"[^a-zA-Z]suite[^a-zA-Z]\", \"[^a-zA-Z]ste[^a-zA-Z]\"],\n",
    "                    [\"[^a-zA-Z]plot[^a-zA-Z]\"],\n",
    "                ],\n",
    "            ]\n",
    "            for each_type in mis_understanding:\n",
    "                change_start = None\n",
    "                change_end = None\n",
    "                for c_id in range(0, len(each_type)):\n",
    "                    # Loop through each class\n",
    "                    c = each_type[c_id]\n",
    "                    for element in c:\n",
    "                        # Find if in string exists an element in each class to replace it by a random element in the other class of the same type\n",
    "                        match = re.search(\"{}\".format(element), target)\n",
    "                        if match is not None:\n",
    "                            change_start = match.start()\n",
    "                            change_end = match.end()\n",
    "                            break\n",
    "                    if change_start is not None:\n",
    "                        # Stop finding if it has founded that exist at least one element in the string and change it\n",
    "                        change_class = c_id\n",
    "                        while change_class == c_id:\n",
    "                            change_class = np.random.randint(0, len(each_type), 1)[0]\n",
    "                        new_class = np.random.choice(each_type[change_class])\n",
    "                        target = (\n",
    "                            target[:change_start]\n",
    "                            + \" \"\n",
    "                            + new_class\n",
    "                            + \" \"\n",
    "                            + target[change_end:]\n",
    "                        )\n",
    "                        target = target.lower()\n",
    "                        # Remove all regex special characters\n",
    "                        target = re.sub(\"\\[\\.\\]\", \"\", target)\n",
    "                        target = re.sub(\"\\[([a\\-z.\\^09]*)\\]\", \"\", target)\n",
    "                        target = re.sub(\"\\{[0-9]+\\}\", \"\", target)\n",
    "                        break\n",
    "\n",
    "        # Typing error (current country -> county)\n",
    "        random = np.random.randint(0, 100, 1)\n",
    "        if random < 41:\n",
    "            match = re.search(\"{}\".format(\"country\"), target)\n",
    "            if match is not None:\n",
    "                change_start = match.start()\n",
    "                change_end = match.end()\n",
    "                target = target[:change_start] + \"county\" + target[change_end:]\n",
    "\n",
    "        address_list[index] = target\n",
    "    temp_df[\"address\"] = address_list\n",
    "    return temp_df\n",
    "\n",
    "\n",
    "def generate_df(df, columns=[\"name\", \"address\"], low=3, high=7):\n",
    "    pos_set, neg_set, full_set = get_classes(df)\n",
    "    # Paring Nos, Pos and Anc together\n",
    "    df_gen = pd.DataFrame()\n",
    "    for cid in pos_set:\n",
    "        # sample 35% of all labeled dataset\n",
    "        df_sample = df.sample(frac=0.01)\n",
    "        # choose rows that have different cid from main one\n",
    "        df_sample = df_sample[~df_sample[\"cid\"].isin([cid])]\n",
    "        df_sample[\"cid\"] = cid\n",
    "        df_sample[\"similar\"] = 0\n",
    "        df_gen = df_gen.append(df_sample)\n",
    "    print(\n",
    "        \"Have done appending {} records \"\n",
    "        \"for {} positive set\".format(len(df_gen), len(pos_set))\n",
    "    )\n",
    "    # ---- Generate new rows by both name and address\n",
    "    result = generate_new_rows(\n",
    "        pd.concat([df, df_gen]).sort_values(\"cid\"),\n",
    "        synonym_dict,\n",
    "        rep_column=columns,\n",
    "        low=low,\n",
    "        high=high,\n",
    "    )\n",
    "    # ---- Append to generated dataframe\n",
    "    df_generated = pd.concat([df, result], ignore_index=True)\n",
    "    df_generated[\"address\"] = df_generated[\"address\"].str.lower()\n",
    "    print(len(df_generated))\n",
    "#     df_generated = pre_processing_pipeline(df_generated)\n",
    "    df = pd.concat([df, df_generated])\n",
    "    \n",
    "    for column in columns:\n",
    "        df[column] = (\n",
    "            df[column]\n",
    "            .str.lower()\n",
    "            .str.replace(\"\\n\", \" \")\n",
    "            .str.replace(r\"[ ]+\", \" \", regex=True)\n",
    "            .str.replace(\"null\", \"\")\n",
    "            .str.replace(\"nan\", \"\")\n",
    "        )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation Mistakes\n",
    "I collect and analyze these mistakes in our labeled data. Then, I use their distribution to generate mistakes automatically to make the original data and generated data look normal.\n",
    "1. Misunderstanding: doing\n",
    "2. Spelling mistake\n",
    "3. Typing error\n",
    "4. System error\n",
    "5. Zipcode error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "21694\n",
      "\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "C:\\Users\\dachu\\Anaconda3\\envs\\short-text-deduplication-siamese\\lib\\site-packages\\ipykernel_launcher.py:143: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4327515b40c942269ef67c78fe4aeda6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = generate_new_rows(df, synonym_dict, rep_column=['address'],low=3, high=5)\n",
    "df_generated = pd.concat([df, result], ignore_index=True)\n",
    "print(len(df_generated))\n",
    "df_generated = pre_processing_pipeline(df_generated)\n",
    "\n",
    "df_generated[\"address\"] = (\n",
    "    df_generated[\"address\"]\n",
    "    .str.lower()\n",
    "    .str.replace(\"\\n\", \" \")\n",
    "    .str.replace(r\"[ ]+\", \" \", regex=True)\n",
    "    .str.replace(\"null\", \"\")\n",
    "    .str.replace(\"nan\", \"\")\n",
    ")\n",
    "df_generated.to_csv(\"data/new_generated_labeled_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open map preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"/data/dac/dedupe-project/openmap-us.csv\", header=None, names=[\"cid\", \"address\"]\n",
    ")\n",
    "\n",
    "df = sklearn.utils.shuffle(df)\n",
    "df[\"similar\"] = 1\n",
    "df_train = df.iloc[:2000, :]\n",
    "df_test = df.iloc[20000:21000, :]\n",
    "\n",
    "if df_train.dtypes[\"cid\"] != \"int64\":\n",
    "    df_train[\"cid\"] = reset_cis(df_train)\n",
    "\n",
    "if df_test.dtypes[\"cid\"] != \"int64\":\n",
    "    df_test[\"cid\"] = reset_cis(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26871004"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Full Positive and Negative\n",
      "2000 Positive only\n",
      "0 Negative only\n",
      "Have done appending 39985 records for 2000 positive set\n",
      "68009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb296a51a424642bd3a59436042f32a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_train_generated = generate_df(df_train, low=2, high=4)\n",
    "df_train_generated.to_csv(\n",
    "    \"/data/dac/dedupe-project/openmap/openmap-us-train.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Full Positive and Negative\n",
      "2000 Positive only\n",
      "0 Negative only\n",
      "Have done appending 39978 records for 2000 positive set\n",
      "43978\n"
     ]
    }
   ],
   "source": [
    "df_test_generated = generate_df(df_train, columns=['address'], low=1, high=2)\n",
    "import copy\n",
    "\n",
    "new_rows = []\n",
    "for row in df_test_generated.itertuples():\n",
    "    # Augment data base on keyboard mistakes and deletion\n",
    "    new_row = list(row)[1:]\n",
    "    aug = nac.KeyboardAug()\n",
    "    augmented_texts = aug.augment(new_row[1], n=1)\n",
    "    new_row[1] = augmented_texts\n",
    "    new_rows.append(copy.copy(new_row))\n",
    "    \n",
    "    new_row = list(row)[1:]\n",
    "    aug = nac.RandomCharAug(action=\"delete\")\n",
    "    augmented_texts = aug.augment(new_row[1], n=1)\n",
    "    new_row[1] = augmented_texts\n",
    "    new_rows.append(copy.copy(new_row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_generated = pd.DataFrame(new_rows, columns=['cid', 'address', 'similar'])\n",
    "df_test_generated.to_csv(\n",
    "    \"/data/dac/dedupe-project/openmap/openmap-us-train.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/data/dac/dedupe-project/test/\"\n",
    "test_df = pd.read_csv(path + \"GT_added.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.drop([\"test_id\", \"id\"], axis=1, inplace=True)\n",
    "test_df[\"similar\"] = 1\n",
    "test_df[\"cid\"] = test_df.index\n",
    "temp_df = test_df\n",
    "temp_df[\"address\"] = temp_df[\"duplicated_address\"]\n",
    "test_df = pd.concat([test_df, temp_df]).loc[:, [\"cid\", \"address\", \"similar\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Full Positive and Negative\n",
      "421 Positive only\n",
      "0 Negative only\n",
      "Have done appending 3357 records for 421 positive set\n",
      "7013\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c8d6f00db54a23b6fcfd6a9c8224ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_df_generated = generate_df(test_df, low=1, high=2)\n",
    "test_df_generated.to_csv(path + \"GT_added_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0d902063ac40bf86d35bbe34440a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=421), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cid_list = test_df.cid.unique()\n",
    "df_1_as_list = []\n",
    "df_2_as_list = []\n",
    "for cid in tqdm(cid_list):\n",
    "    # Generate two dataset with label to compare\n",
    "    df = test_df[test_df.cid == cid]\n",
    "\n",
    "    # Positive case\n",
    "    pos = df[df.similar == 1]\n",
    "    df_2_as_list.append(pos.iloc[1:, :])\n",
    "    [df_1_as_list.append(pos.iloc[0, :]) for i in range(0, len(pos.iloc[1:, :]))]\n",
    "\n",
    "    # Negative case\n",
    "    neg = df[df.similar == 0]\n",
    "    df_2_as_list.append(neg.iloc[0:, :])\n",
    "    [df_1_as_list.append(pos.iloc[0, :]) for i in range(0, len(neg.iloc[0:, :]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.concat(df_1_as_list, axis=1).T\n",
    "df_2 = pd.concat(df_2_as_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.to_csv(path + \"GT_added_new_anchor.csv\")\n",
    "df_2.to_csv(path + \"GT_added_new_check.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}