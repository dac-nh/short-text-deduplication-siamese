{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load library and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package punkt to /home/dac/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from torch import optim\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from model.model import TripletModel, TripletNoEmbeddingModel\n",
    "from model.SelfAttentionModel import StructuredSelfAttention\n",
    "from utils.data_loader import load_data_set, load_word_to_index, load_bow_data, load_triplet_orders, load_padded_data, load_triplet\n",
    "from utils.pretrained_glove_embeddings import load_glove_embeddings\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load dataset successfully!\n",
      "Loading triplet order successfully!\n"
     ]
    }
   ],
   "source": [
    "full_generated_data_path = 'generated_labeled_data.csv'\n",
    "# device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# Load dataset\n",
    "df = load_data_set(full_generated_data_path, retrain=False)\n",
    "print('Load dataset successfully!')\n",
    "\n",
    "df_triplet_orders = load_triplet_orders(df, retrain=False)['content']\n",
    "print('Loading triplet order successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28219    inactive - riviera home furnishings pvt. ltd. ...\n",
       "2320       3pl global 2211 e carson carson usa 90810 90810\n",
       "4125     inactive -jiangyin snowballion textile industr...\n",
       "32632    colorland co. ltd. # 545-2 dongduchun-dong str...\n",
       "36949    aerofil technology inc 225 industrial park dr....\n",
       "                               ...                        \n",
       "20455    al-karam textile mills (pvt) limited h.t.11/1 ...\n",
       "27237    pactiv - can 2480 sommers drive canandaigua us...\n",
       "48922           jang you inc. 620-5 choji-dong ansansi kr \n",
       "7176     elite comfort solutions inc. 24 herring rd new...\n",
       "16085    delta galil 3601 west 4th street williamsport ...\n",
       "Name: content, Length: 85064, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle(content_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arunima PROPN\n",
      "sports NOUN\n",
      "wear VERB\n",
      "limited ADJ\n",
      "dewan PROPN\n",
      "idris PROPN\n",
      "rd PROPN\n",
      ". PUNCT\n",
      "zirabo PROPN\n",
      "savar PROPN\n",
      "null PROPN\n",
      "ashulia PROPN\n",
      "bd PROPN\n",
      "null PROPN\n",
      "nguyen PROPN\n",
      "toan PROPN\n",
      "trading PROPN\n",
      "& CCONJ\n",
      "apparel PROPN\n",
      "co. PROPN\n",
      "ltd PROPN\n",
      ". PROPN\n",
      "chau PROPN\n",
      "son PROPN\n",
      "industrial PROPN\n",
      "zone PROPN\n",
      "chau PROPN\n",
      "son PROPN\n",
      "ward PROPN\n",
      "phu PROPN\n",
      "ly PROPN\n",
      "ha INTJ\n",
      "nam PROPN\n",
      "1 NUM\n",
      "guangdong PROPN\n",
      "midea PROPN\n",
      "kitchen PROPN\n",
      "appliances NOUN\n",
      "manufacturing VERB\n",
      "no DET\n",
      "6 NUM\n",
      "yong NOUN\n",
      "an DET\n",
      "road NOUN\n",
      "beijiao PROPN\n",
      "shunde PROPN\n",
      "foshan PROPN\n",
      "guangdong PROPN\n",
      "foshan PROPN\n",
      "cn PROPN\n",
      "528311 NUM\n",
      "528311 NUM\n",
      "pt PROPN\n",
      ". PUNCT\n",
      "anugerah PROPN\n",
      "abadi PROPN\n",
      "magelang PROPN\n",
      "dusun PROPN\n",
      "demesan PROPN\n",
      "rt.004 PROPN\n",
      "rw.002 PROPN\n",
      "desa NOUN\n",
      "girirejo NOUN\n",
      "kec.tempuran PROPN\n",
      "kab.magelang PROPN\n",
      "inactive ADJ\n",
      "- PUNCT\n",
      "fujian PROPN\n",
      "jinjiang PROPN\n",
      "city PROPN\n",
      "fulian PROPN\n",
      "shoes NOUN\n",
      "& CCONJ\n",
      "plastics PROPN\n",
      "company PROPN\n",
      "ltd PROPN\n",
      "huzhong PROPN\n",
      "industry PROPN\n",
      "area PROPN\n",
      "chendai PROPN\n",
      "362211 NUM\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "sp = spacy.load('en')\n",
    "\n",
    "for content in shuffle(content_df)[:5]:\n",
    "    for word in sp(content):\n",
    "        print(word.text, word.pos_)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_size = 20  # pad size or timestep\n",
    "batch_size = 10000\n",
    "embedding = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split():\n",
    "    print('Train/test split')\n",
    "    train_X_df = df[df['cid'].isin(df['cid'].unique()[0:-10])]\n",
    "    train_df_triplet_orders = load_triplet_orders(train_X_df, dump_path='triplet_samples_id_train.csv',\n",
    "                                                  retrain=False)['content']\n",
    "    train_anc_loader, train_pos_loader, train_neg_loader = load_triplet(x_padded, train_df_triplet_orders,\n",
    "                                                                        dump_path='embedding/triplet_data_train.pkl',\n",
    "                                                                        batch_size=batch_size, retrain=True)\n",
    "    test_X_df = df[df['cid'].isin(df['cid'].unique()[-10:])]\n",
    "    test_df_triplet_orders = load_triplet_orders(test_X_df, dump_path='triplet_samples_id_test.csv',\n",
    "                                                 retrain=False)['content']\n",
    "    test_anc_loader, test_pos_loader, test_neg_loader = load_triplet(x_padded, test_df_triplet_orders,\n",
    "                                                                     dump_path='embedding/triplet_data_test.pkl',\n",
    "                                                                     batch_size=batch_size, retrain=true)\n",
    "    print('All train/test sets are ready!')\n",
    "\n",
    "    return (train_anc_loader, train_pos_loader, train_neg_loader), (test_anc_loader, test_pos_loader, test_neg_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load word to index successfully!\n",
      "Load pretrained embedding\n",
      "Load padded data successfully!\n",
      "Load triplet data successfully!\n"
     ]
    }
   ],
   "source": [
    "# get word to index and embedding whole dataset\n",
    "word_to_index = load_word_to_index(df, retrain=False)\n",
    "print('Load word to index successfully!')\n",
    "embeddings = load_glove_embeddings(word_to_index, embedding_dim=300, retrain=False)\n",
    "print('Load pretrained embedding')\n",
    "X = load_padded_data(df, word_to_index, pad_size=pad_size, retrain=False)\n",
    "print('Load padded data successfully!')\n",
    "if embedding: \n",
    "    # Pre-embedding to not update embedding through all training data\n",
    "    X = embedding_data(X, embeddings)\n",
    "    print('Embedding data sucessfully!')\n",
    "anc_loader, pos_loader, neg_loader = load_triplet(X, df_triplet_orders,\n",
    "                                                  batch_size=batch_size, \n",
    "                                                  retrain=False)\n",
    "print('Load triplet data successfully!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention triplet model with embedding inside model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredSelfAttention(\n",
       "  (embeddings): Embedding(1890, 300)\n",
       "  (lstm): LSTM(300, 120, batch_first=True)\n",
       "  (linear_first): Linear(in_features=120, out_features=100, bias=True)\n",
       "  (linear_second): Linear(in_features=100, out_features=10, bias=True)\n",
       "  (linear_final): Linear(in_features=120, out_features=50, bias=True)\n",
       "  (linear_distance): Linear(in_features=100, out_features=1, bias=True)\n",
       "  (tanh): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Self-attention triplet model\n",
    "lr = 0.1\n",
    "margin = 0.2\n",
    "attentive_features = 10\n",
    "# Load model & optimizer\n",
    "model = StructuredSelfAttention(embeddings=embeddings, max_len=pad_size,\n",
    "                                               r=attentive_features, margin=margin, \n",
    "                                               cuda=device).cuda(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredSelfAttention(\n",
       "  (embeddings): Embedding(1890, 300)\n",
       "  (lstm): LSTM(300, 120, batch_first=True)\n",
       "  (linear_first): Linear(in_features=120, out_features=100, bias=True)\n",
       "  (linear_second): Linear(in_features=100, out_features=40, bias=True)\n",
       "  (linear_final): Linear(in_features=120, out_features=50, bias=True)\n",
       "  (linear_distance): Linear(in_features=100, out_features=1, bias=True)\n",
       "  (tanh): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model and optimizer\n",
    "self_attention_model_path = '/data/dac/dedupe-project/model/sam_nls_300d_40p_notest'\n",
    "\n",
    "checkpoint = torch.load(model)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "524dede1792544cda3eddca766a3880a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0\t\t\t\tAverage Loss:\t0.1934\t\tAvg Accuracy:\t0.548753\t\t\t\t\n",
      "Epoch:1\t\t\t\tAverage Loss:\t0.0995\t\tAvg Accuracy:\t0.85989\t\t\t\t\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-071f925860f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mavg_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manc_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_x\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manc_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Training model per batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Send data to graphic card - Cuda0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dedupe-project/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dedupe-project/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/dedupe-project/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dedupe-project/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dedupe-project/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "epochs = 10\n",
    "best_lost = None\n",
    "clipping_value = 1\n",
    "loss_list = []\n",
    "average_list = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in tqdm(range(epochs), desc='Epoch'):\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    for batch, [anc_x, pos_x, neg_x] in enumerate(zip(anc_loader, pos_loader, neg_loader)):\n",
    "        # Training model per batch\n",
    "        # Send data to graphic card - Cuda0\n",
    "        anc_x, pos_x, neg_x = anc_x[0].to(device), pos_x[0].to(device), neg_x[0].to(device)\n",
    "        pos_pred, neg_pred = model(anc_x, pos_x, neg_x)\n",
    "\n",
    "        loss = (pos_pred + neg_pred).mean()\n",
    "        corrects = torch.sum(pos_pred == 0) + torch.sum(neg_pred == 0)\n",
    "        accuracy = float(corrects) / (2 * len(anc_x))\n",
    "        avg_acc += accuracy\n",
    "        avg_loss += float(loss)\n",
    "        # Gradient\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()  # Empty cuda cache\n",
    "        print('\\rBatch:\\t{}\\t\\tLoss:\\t{}\\t\\tAccuracy:\\t{}\\t\\t'.format(batch, float(loss),\n",
    "                                                                      round(accuracy, 4)), end='')\n",
    "    # Average loss and accuracy\n",
    "    avg_acc = avg_acc / len(anc_loader)\n",
    "    avg_loss = avg_loss / len(anc_loader)\n",
    "    loss_list.append(avg_loss)\n",
    "    average_list.append(avg_acc)\n",
    "    print('\\rEpoch:{}\\t\\t\\t\\tAverage Loss:\\t{}\\t\\tAvg Accuracy:\\t{}'.format(epoch, round(avg_loss, 4),\n",
    "                                                                            round(avg_acc, 4)))\n",
    "    if best_lost is None or best_lost > loss:\n",
    "        best_lost = loss\n",
    "print(\"--- %s seconds ---\"%(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "self_attention_model_path = '/data/dac/dedupe-project/model/sam_nls_300d_40p_notest'\n",
    "\n",
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict()\n",
    "}, self_attention_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TripletModel with embedding inside model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TripletModel(\n",
       "  (embeddings): Embedding(1890, 300)\n",
       "  (lstm): GRU(300, 120, batch_first=True, bidirectional=True)\n",
       "  (linear_final): Linear(in_features=240, out_features=50, bias=True)\n",
       "  (linear_distance): Linear(in_features=100, out_features=1, bias=True)\n",
       "  (tanh): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Self-attention triplet model\n",
    "# triplet_300d_20p_dynamic_embedding -- glove embedding\n",
    "# triplet_300d_20p_own_embedding -- initiate embedding\n",
    "# triplet_300d_20p_own_embedding_bi_gru -- gru\n",
    "# triplet_300d_20p_own_embedding_bi -- lstm\n",
    "triplet_model_path = '/data/dac/dedupe-project/model/triplet_300d_20p_own_embedding_bi_gru'\n",
    "\n",
    "lr = 0.0002\n",
    "margin = 0.2\n",
    "# Load model & optimizer\n",
    "model = TripletModel(embeddings=embeddings, max_len=pad_size,\n",
    "                     margin=margin, cuda=device).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TripletModel(\n",
       "  (embeddings): Embedding(1890, 300)\n",
       "  (lstm): GRU(300, 120, batch_first=True, bidirectional=True)\n",
       "  (linear_final): Linear(in_features=240, out_features=50, bias=True)\n",
       "  (linear_distance): Linear(in_features=100, out_features=1, bias=True)\n",
       "  (tanh): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model and optimizer\n",
    "checkpoint = torch.load(triplet_model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d7fdbf592e5495eb7ac32fc118f1c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:\t0\t\tAverage Loss:\t0.9152\t\tAvg Accuracy:\t0.5332\t\t\n",
      "Batch:\t96\t\tLoss:\t0.9049\t\tAccuracy:\t0.4618\t\t"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "epochs = 10\n",
    "best_lost = None\n",
    "early_stopping_steps = 7\n",
    "forward_index = 0\n",
    "\n",
    "loss_list = []\n",
    "average_list = []\n",
    "model.train()\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in tqdm(range(epochs), desc='Epoch'):\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    for batch, [anc_x, pos_x, neg_x] in enumerate(zip(anc_loader, pos_loader, neg_loader)):\n",
    "        # Training model per batch\n",
    "        # Send data to graphic card - Cuda0\n",
    "        anc_x, pos_x, neg_x = anc_x[0].to(device), pos_x[0].to(device), neg_x[0].to(device)\n",
    "        pos_pred, neg_pred = model(anc_x, pos_x, neg_x)\n",
    "\n",
    "        loss = (pos_pred + neg_pred).mean()\n",
    "        corrects = torch.sum(pos_pred == 0) + torch.sum(neg_pred == 0)\n",
    "        accuracy = float(corrects) / (2 * len(anc_x))\n",
    "        avg_acc += accuracy\n",
    "        avg_loss += float(loss)\n",
    "\n",
    "        # Gradient\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()  # Empty cuda cache\n",
    "        print('\\rBatch:\\t{}\\t\\tLoss:\\t{}\\t\\tAccuracy:\\t{}\\t\\t'.format(batch, round(float(loss), 4),\n",
    "                                                                      round(accuracy, 4)), end='')\n",
    "    # Average loss and accuracy\n",
    "    avg_acc = avg_acc / len(anc_loader)\n",
    "    avg_loss = avg_loss / len(anc_loader)\n",
    "    loss_list.append(avg_loss)\n",
    "    average_list.append(avg_acc)\n",
    "    print('\\rEpoch:\\t{}\\t\\tAverage Loss:\\t{}\\t\\tAvg Accuracy:\\t{}\\t\\t'.format(epoch, round(avg_loss, 4),\n",
    "                                                                            round(avg_acc, 4)))\n",
    "    if avg_acc > 0.87:\n",
    "        break\n",
    "    if best_lost is None or best_lost > avg_loss:\n",
    "        best_lost = avg_loss\n",
    "        forward_index = 0\n",
    "        \n",
    "        # Save model\n",
    "#         torch.save({\n",
    "#             'model': model.state_dict(),\n",
    "#             'optimizer': optimizer.state_dict()\n",
    "#         }, triplet_model_path)\n",
    "    else:\n",
    "        # Early stopping after reachs {early_stopping_steps} steps\n",
    "        forward_index += 1\n",
    "        if forward_index == early_stopping_steps:\n",
    "            break\n",
    "        \n",
    "print(\"--- %s seconds ---\"%(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet Model without embedding inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Self-attention triplet model\n",
    "triplet_ne_model_path = '/data/dac/dedupe-project/model/triplet_300d_20p_ne'\n",
    "\n",
    "lr = 0.002\n",
    "margin = 0.1\n",
    "# Load model & optimizer\n",
    "train_ne_model = TripletModelExEmb(embeddings=embeddings, max_len=pad_size,\n",
    "                     margin=margin, cuda=cuda0).cuda(cuda0)\n",
    "optimizer = optim.Adam(train_ne_model.parameters(), lr=lr)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load model and optimizer\n",
    "checkpoint = torch.load(triplet_ne_model_path)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "epochs = 40\n",
    "best_lost = None\n",
    "early_stopping_steps = 5\n",
    "forward_index = 0\n",
    "\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "model.train()\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in tqdm(range(epochs), desc='Epoch'):\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    for batch, [anc_x, pos_x, neg_x] in enumerate(zip(anc_loader, pos_loader, neg_loader)):\n",
    "        # Training model per batch\n",
    "        # Send data to graphic card - Cuda0\n",
    "        anc_x, pos_x, neg_x = anc_x[0].to(device), pos_x[0].to(device), neg_x[0].to(device)\n",
    "        pos_pred, neg_pred = model(anc_x, pos_x, neg_x)\n",
    "\n",
    "        loss = (pos_pred + neg_pred).mean()\n",
    "        corrects = torch.sum(pos_pred == 0) + torch.sum(neg_pred == 0)\n",
    "        accuracy = float(corrects) / (2 * len(anc_x))\n",
    "        avg_acc += accuracy\n",
    "        avg_loss += float(loss)\n",
    "\n",
    "        # Gradient\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         torch.cuda.empty_cache()  # Empty cuda cache\n",
    "        print('\\rBatch:\\t{}\\t\\tLoss:\\t{}\\t\\tAccuracy:\\t{}\\t\\t'.format(batch, round(float(loss), 4),\n",
    "                                                                      round(accuracy, 4)), end='')\n",
    "    # Average loss and accuracy\n",
    "    avg_acc = avg_acc / len(anc_loader)\n",
    "    avg_loss = avg_loss / len(anc_loader)\n",
    "    loss_list.append(avg_loss)\n",
    "    average_list.append(avg_acc)\n",
    "    print('\\rEpoch:\\t{}\\t\\tAverage Loss:\\t{}\\t\\tAvg Accuracy:\\t{}\\t\\t'.format(epoch, round(avg_loss, 4),\n",
    "                                                                            round(avg_acc, 4)))\n",
    "    if best_lost is None or best_lost > avg_loss:\n",
    "        best_lost = avg_loss\n",
    "        forward_index = 0\n",
    "        \n",
    "        # Save model\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "        }, triplet_model_path)\n",
    "    else:\n",
    "        # Early stopping after reachs {early_stopping_steps} steps\n",
    "        forward_index += 1\n",
    "        if forward_index == early_stopping_steps:\n",
    "            break\n",
    "        \n",
    "print(\"--- %s seconds ---\"%(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet Bag of Word Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load bag of word data successfully!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8b99a7ca5f48b883a15dfd104fdeac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Load triplets', max=1644502, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Bag of word\n",
    "batch_size = 200\n",
    "x_train_bow = np.array(load_bow_data(df, word_to_index, retrain=True))\n",
    "x_train_bow = x_train_bow[:, :, x_train_bow.sum(0)[0]>250]\n",
    "print('Load bag of word data successfully!')\n",
    "\n",
    "anc_loader, pos_loader, neg_loader = load_triplet(x_train_bow, df_triplet_orders,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  dump_path='embedding/triplet__bow_data.pkl',\n",
    "                                                  embedded=True,\n",
    "                                                  retrain=True)\n",
    "vocabulary_size = len(x_train_bow[0])\n",
    "print('Load triplet data successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Self-attention triplet model\n",
    "triplet_model_path = '/data/dac/dedupe-project/model/triplet_bow'\n",
    "cuda0 = torch.device('cuda:0')\n",
    "\n",
    "lr = 0.0001\n",
    "margin = 0.2\n",
    "# Load model & optimizer\n",
    "model = TripletBoWModel(max_len=vocabulary_size,\n",
    "                        margin=margin, cuda=cuda0).cuda(cuda0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "epochs = 40\n",
    "best_lost = None\n",
    "early_stopping_steps = 3\n",
    "forward_index = 0\n",
    "\n",
    "loss_list = []\n",
    "average_list = []\n",
    "model.train()\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in tqdm(range(epochs), desc='Epoch'):\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    for batch, [anc_x, pos_x, neg_x] in enumerate(zip(anc_loader, pos_loader, neg_loader)):\n",
    "        # Training model per batch\n",
    "        # Send data to graphic card - Cuda0\n",
    "        anc_x, pos_x, neg_x = anc_x[0].to(cuda0), pos_x[0].to(cuda0), neg_x[0].to(cuda0)\n",
    "        pos_pred, neg_pred = model(anc_x, pos_x, neg_x)\n",
    "\n",
    "        loss = (pos_pred + neg_pred).mean()\n",
    "        corrects = torch.sum(pos_pred == 0) + torch.sum(neg_pred == 0)\n",
    "        accuracy = float(corrects) / (2 * len(anc_x))\n",
    "        avg_acc += accuracy\n",
    "        avg_loss += float(loss)\n",
    "        # Gradient\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()  # Empty cuda cache\n",
    "        print('\\rBatch:\\t{}\\t\\tLoss:\\t{}\\t\\tAccuracy:\\t{}\\t\\t'.format(batch, round(float(loss), 4),\n",
    "                                                                      round(accuracy, 4)), end='')\n",
    "    # Average loss and accuracy\n",
    "    avg_acc = avg_acc / len(anc_loader)\n",
    "    avg_loss = avg_loss / len(anc_loader)\n",
    "    loss_list.append(avg_loss)\n",
    "    average_list.append(avg_acc)\n",
    "    print('\\rEpoch:\\t{}\\t\\tAverage Loss:\\t{}\\t\\tAvg Accuracy:\\t{}\\t\\t'.format(epoch, round(avg_loss, 4),\n",
    "                                                                            round(avg_acc, 4)))\n",
    "    if best_lost is None or best_lost > avg_loss:\n",
    "        best_lost = avg_loss\n",
    "        forward_index = 0\n",
    "        \n",
    "#         # Save model\n",
    "#         torch.save({\n",
    "#             'model': model.state_dict(),\n",
    "#             'optimizer': optimizer.state_dict()\n",
    "#         }, triplet_model_path)\n",
    "    else:\n",
    "        # Early stopping after reachs {early_stopping_steps} steps\n",
    "        forward_index += 1\n",
    "        if forward_index == early_stopping_steps:\n",
    "            break\n",
    "        \n",
    "print(\"--- %s seconds ---\"%(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentPoolEmbeddings, BertEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "glove_embedding = WordEmbeddings('glove').cuda(device)\n",
    "flair_embedding_news_forward = FlairEmbeddings('news-forward-fast').cuda(device)\n",
    "flair_embedding_news_backward = FlairEmbeddings('news-backward-fast').cuda(device)\n",
    "bert_embedding = BertEmbeddings().cuda(device)\n",
    "\n",
    "document_embeddings = DocumentPoolEmbeddings([\n",
    "    flair_embedding_news_forward, \n",
    "    flair_embedding_news_backward\n",
    "])\n",
    "\n",
    "\n",
    "emb_dim = 2048\n",
    "\n",
    "def stack_embedding(row, document_embeddings):\n",
    "    sentence = Sentence(row)\n",
    "    z = sentence.embedding.size()[0]\n",
    "    document_embeddings.embed(sentence)\n",
    "    \n",
    "    return sentence.embedding.view(1, emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TripletNoEmbeddingModel(\n",
       "  (lstm): LSTM(2048, 120, batch_first=True)\n",
       "  (linear_final): Linear(in_features=120, out_features=50, bias=True)\n",
       "  (linear_distance): Linear(in_features=100, out_features=1, bias=True)\n",
       "  (tanh): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Self-attention triplet model\n",
    "se_model_path = '/data/dac/dedupe-project/model/triplet_stacked_embedding'\n",
    "\n",
    "lr = 0.01\n",
    "margin = 0.1\n",
    "# Load model & optimizer\n",
    "model = TripletNoEmbeddingModel(max_len=1,\n",
    "                     margin=margin, cuda=device, emb_dim=emb_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7881f6e3f24818b618e80ca63d7e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=5, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:\t2042\t\tLoss:\t0.0968\t\tAccuracy:\t0.956\t\t"
     ]
    }
   ],
   "source": [
    "batch = torch.zeros([0, emb_dim])\n",
    "batch_size = 500\n",
    "current_size = 0\n",
    "best_lost = None\n",
    "epochs = 1\n",
    "\n",
    "model.train()\n",
    "\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "start_time = time.time()\n",
    "for epoch in tqdm(range(epochs), desc='Epoch'):\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    count = 0\n",
    "    \n",
    "    anc_x = torch.zeros([0, emb_dim])\n",
    "    pos_x = torch.zeros([0, emb_dim])\n",
    "    neg_x = torch.zeros([0, emb_dim])\n",
    "    for index, row in df_triplet_orders.iterrows():\n",
    "        anc_loc, pos_loc, neg_loc = row[['anchor', 'pos', 'neg']]\n",
    "        \n",
    "        anc = df.loc[anc_loc, 'content'][0]\n",
    "        pos = df.loc[pos_loc, 'content'][0]\n",
    "        neg = df.loc[neg_loc, 'content'][0]\n",
    "        anc = stack_embedding(anc, document_embeddings)\n",
    "        pos = stack_embedding(pos, document_embeddings)\n",
    "        neg = stack_embedding(neg, document_embeddings)\n",
    "        \n",
    "        if current_size < batch_size:\n",
    "            # Create DataLoader with batchsize is 500\n",
    "            anc_x = torch.cat((anc_x, anc.cpu()), 0)\n",
    "            pos_x = torch.cat((pos_x, pos.cpu()), 0)\n",
    "            neg_x = torch.cat((neg_x, neg.cpu()), 0)\n",
    "            current_size += 1\n",
    "        else:\n",
    "            # Training model per batch\n",
    "            anc_x, pos_x, neg_x = anc_x.to(device), pos_x.to(device), neg_x.to(device)\n",
    "            pos_pred, neg_pred = model(anc_x, pos_x, neg_x)\n",
    "\n",
    "            loss = (pos_pred + neg_pred).mean()\n",
    "            corrects = torch.sum(pos_pred == 0) + torch.sum(neg_pred == 0)\n",
    "            accuracy = float(corrects) / (2 * len(anc_x))\n",
    "            avg_acc += accuracy\n",
    "            avg_loss += float(loss)\n",
    "            # Gradient\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            torch.cuda.empty_cache()  # Empty cuda cache\n",
    "            print('\\rBatch:\\t{}\\t\\tLoss:\\t{}\\t\\tAccuracy:\\t{}\\t\\t'.format(count, round(float(loss), 4),\n",
    "                                                                      round(accuracy, 4)), end='')\n",
    "            \n",
    "            # Reset data loader and increase count\n",
    "            current_size = 1\n",
    "            anc_x = anc.cpu()\n",
    "            pos_x = pos.cpu()\n",
    "            neg_x = neg.cpu()\n",
    "            count += 1\n",
    "            \n",
    "    # Average loss and accuracy\n",
    "    avg_acc = avg_acc / (len(df)/batch_size)\n",
    "    avg_loss = avg_loss / (len(df)/batch_size)\n",
    "    loss_list.append(avg_loss)\n",
    "    acc_list.append(avg_acc)\n",
    "    print('\\rEpoch:\\t1{}\\t\\tAverage Loss:\\t{}\\t\\tAvg Accuracy:\\t{}\\t\\t'.format(epoch, round(avg_loss, 4),\n",
    "                                                                            round(avg_acc, 4)))\n",
    "    if best_lost is None or best_lost > avg_loss:\n",
    "        best_lost = avg_loss\n",
    "        forward_index = 0\n",
    "        \n",
    "#         # Save model\n",
    "#         torch.save({\n",
    "#             'model': model.state_dict(),\n",
    "#             'optimizer': optimizer.state_dict()\n",
    "#         }, triplet_model_path)\n",
    "    else:\n",
    "        # Early stopping after reachs {early_stopping_steps} steps\n",
    "        forward_index += 1\n",
    "        if forward_index == early_stopping_steps:\n",
    "            break\n",
    "        \n",
    "print(\"--- %s seconds ---\"%(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "avg_loss = 0\n",
    "avg_acc = 0\n",
    "\n",
    "y_true = np.array([])\n",
    "y_pred = np.array([])\n",
    "\n",
    "for batch, [anc_x, pos_x, neg_x] in enumerate(zip(test_anc_loader, test_pos_loader, test_neg_loader)):\n",
    "    # Send data to graphic card - Cuda0\n",
    "    anc_x, pos_x, neg_x = anc_x[0].to(cuda0), pos_x[0].to(cuda0), neg_x[0].to(cuda0)\n",
    "    with torch.no_grad():\n",
    "        pos_pred, neg_pred = model(anc_x, pos_x, neg_x)\n",
    "        pos_pred, neg_pred = pos_pred.cpu(), neg_pred.cpu()\n",
    "        corrects = torch.sum(pos_pred == 0) + torch.sum(neg_pred == 0)\n",
    "\n",
    "        y_true_curr = np.concatenate([np.ones(len(pos_pred)), np.zeros(len(neg_pred))])\n",
    "        y_true = np.concatenate([y_true, y_true_curr])\n",
    "        \n",
    "        y_pred_curr = np.concatenate([np.ones(len(pos_pred)), np.zeros(len(neg_pred))])\n",
    "        y_pred_curr[np.where(pos_pred != 0)[0]] = 0\n",
    "        y_pred_curr[np.where(neg_pred != 0)[0] + len(pos_pred)] = 1\n",
    "        y_pred = np.concatenate([y_pred, y_pred_curr])\n",
    "        print('\\rBatch:\\t{}\\t\\tAccuracy:\\t{}\\t\\tF1-score:\\t{}\\t\\t'.format(\n",
    "            batch, round(accuracy_score(y_true_curr, y_pred_curr), 4),\n",
    "            round(f1_score(y_true_curr, y_pred_curr), 4)), end='')\n",
    "print('\\nAvg Accuracy:\\t{}\\t\\t\\tAvg F1-score:\\t{}\\t\\t'.format(\n",
    "    round(accuracy_score(y_true, y_pred), 4), round(f1_score(y_true, y_pred), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/dac/dedupe-project/test/'\n",
    "test_df = pd.read_excel(path + 'GT_added.xls')\n",
    "test_df.fillna('', inplace=True)\n",
    "# test_df = shuffle(test_df)\n",
    "test_df_1 = test_df.loc[:, ['name', 'address']]\n",
    "test_df_1['content'] = test_df_1['name'].str.lower() + ' ' + test_df_1['address'].str.lower()\n",
    "test_df_1['content'] = test_df_1['content'].str.replace('\\n', ' ').str.replace(',' ,' ').str.replace(r'[ ]+', ' ', regex=True)\n",
    "test_df_2 = test_df.loc[:, ['duplicated_name', 'duplicated_address']]\n",
    "test_df_2['content'] = test_df_2['duplicated_name'].str.lower() + ' ' + test_df_2['duplicated_address'].str.lower()\n",
    "test_df_2['content'] = test_df_2['content'].str.replace('\\n', ' ').str.replace(',' ,' ').str.replace(r'[ ]+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd5a50a4da2d49e099dc66483ae194ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Padding', max=447, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3343d05a45df4880bdddabb76afba418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Padding', max=447, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64baece7ceff4431b587dde1dcd40add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:\t0.9732\t\tF1-score:\t0.9864\t\t"
     ]
    }
   ],
   "source": [
    "x1 = load_padded_data(pd.DataFrame(test_df_1), word_to_index, dump_path=None,\n",
    "                                   pad_size=pad_size, retrain=True)\n",
    "x2 = load_padded_data(pd.DataFrame(test_df_2), word_to_index, dump_path=None, \n",
    "                                   pad_size=pad_size, retrain=True)\n",
    "\n",
    "def create_data_loader(array, batch_size=batch_size):\n",
    "    # Create data loader\n",
    "    data = TensorDataset(torch.from_numpy(array).type(torch.LongTensor))\n",
    "    loader = DataLoader(data, batch_size=batch_size, drop_last=False)\n",
    "    return loader\n",
    "\n",
    "x1 = create_data_loader(x1)\n",
    "x2 = create_data_loader(x2)\n",
    "\n",
    "pred_list = np.array([])\n",
    "y_true = np.array([])\n",
    "y_pred = np.array([])\n",
    "att1_list = []\n",
    "att2_list = []\n",
    "for a, b in tqdm(zip(x1, x2)):\n",
    "    # Send data to graphic card - Cuda0\n",
    "    a, b = a[0].to(device), b[0].to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = model(a, b)\n",
    "        pred = pred.cpu()\n",
    "#         att1 = att1.cpu()\n",
    "#         att2 = att2.cpu()\n",
    "        y_true_curr = np.ones(len(pred))\n",
    "        y_true = np.concatenate([y_true, y_true_curr])\n",
    "\n",
    "        y_pred_curr = np.ones(len(pred))\n",
    "        y_pred_curr[np.where(pred <= 0)[0]] = 0\n",
    "        y_pred = np.concatenate([y_pred, y_pred_curr])\n",
    "        \n",
    "        pred_list = np.concatenate([pred_list, pred.squeeze().data.numpy()])\n",
    "#         att1_list.append(att1)\n",
    "#         att2_list.append(att2)\n",
    "\n",
    "print('Accuracy:\\t{}\\t\\tF1-score:\\t{}\\t\\t'.format(\n",
    "    round(accuracy_score(y_true, y_pred), 4),\n",
    "    round(f1_score(y_true, y_pred), 4)), end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test(n):\n",
    "    test_df_1a = pd.DataFrame()\n",
    "    test_df_1b = pd.DataFrame()\n",
    "\n",
    "    for i1, i2 in list(itertools.combinations(test_df_1.index, 2))[:n]:\n",
    "        test_df_1a = test_df_1a.append(test_df_1.iloc[i1, :])\n",
    "        test_df_1b = test_df_1b.append(test_df_1.iloc[i2, :])\n",
    "\n",
    "    test_df_1b = test_df_1b.append(test_df_1a)\n",
    "    test_df_1a = test_df_1a.append(test_df_1a)\n",
    "\n",
    "    test_df_1a.reset_index(inplace=True)\n",
    "    test_df_1b.reset_index(inplace=True)\n",
    "    \n",
    "    return test_df_1a, test_df_1b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_n = 500\n",
    "test_df_1a, test_df_1b = create_test(test1_n)\n",
    "\n",
    "def create_data_loader(array, batch_size=batch_size):\n",
    "    # Create data loader\n",
    "    data = TensorDataset(torch.from_numpy(array).type(torch.LongTensor))\n",
    "    loader = DataLoader(data, batch_size=batch_size, drop_last=False)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6f10d88074437c94b97ce6754df545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Padding', max=1000, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f0409b81f3843529e620652116400ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Padding', max=1000, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b48264bd6f6b4af2a63f1c9782aba270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:\t0.66\t\tF1-score:\t0.7463\t\t"
     ]
    }
   ],
   "source": [
    "x1 = load_padded_data(pd.DataFrame(test_df_1a), word_to_index, dump_path=None,\n",
    "                                   pad_size=pad_size, retrain=True)\n",
    "x2 = load_padded_data(pd.DataFrame(test_df_1b), word_to_index, dump_path=None, \n",
    "                                   pad_size=pad_size, retrain=True)\n",
    "\n",
    "x1 = create_data_loader(x1)\n",
    "x2 = create_data_loader(x2)\n",
    "\n",
    "pred_list = np.array([])\n",
    "y_true = np.array([])\n",
    "y_pred = np.array([])\n",
    "# att1_list = []\n",
    "# att2_list = []\n",
    "for a, b in tqdm(zip(x1, x2)):\n",
    "    # Send data to graphic card - Cuda0\n",
    "    a, b = a[0].to(device), b[0].to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = model(a, b)\n",
    "        pred = pred.cpu()\n",
    "#         att1 = att1.cpu()\n",
    "#         att2 = att2.cpu()\n",
    "        y_true_curr = np.zeros(len(pred))\n",
    "        y_true_curr[test1_n:] = 1\n",
    "        y_true = np.concatenate([y_true, y_true_curr])\n",
    "\n",
    "        y_pred_curr = np.ones(len(pred))\n",
    "        y_pred_curr[np.where(pred <= 0)[0]] = 0\n",
    "        y_pred = np.concatenate([y_pred, y_pred_curr])\n",
    "        \n",
    "        pred_list = np.concatenate([pred_list, pred.squeeze().data.numpy()])\n",
    "#         att1_list.append(att1)\n",
    "#         att2_list.append(att2)\n",
    "\n",
    "print('Accuracy:\\t{}\\t\\tF1-score:\\t{}\\t\\t'.format(\n",
    "    round(accuracy_score(y_true, y_pred), 4),\n",
    "    round(f1_score(y_true, y_pred), 4)), end='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_n = 1000\n",
    "test_df_1a, test_df_1b = create_test(test2_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0619a087f55f45ecaf2edc1eeaaeff15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Padding', max=2000, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c098102f1532465d9a2bcfe38e1a85f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Padding', max=2000, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39d695e2486747839e6599abbb3935a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:\t0.758\t\tF1-score:\t0.8052\t\t"
     ]
    }
   ],
   "source": [
    "x1 = load_padded_data(pd.DataFrame(test_df_1a), word_to_index, dump_path=None,\n",
    "                                   pad_size=pad_size, retrain=True)\n",
    "x2 = load_padded_data(pd.DataFrame(test_df_1b), word_to_index, dump_path=None, \n",
    "                                   pad_size=pad_size, retrain=True)\n",
    "\n",
    "def create_data_loader(array, batch_size=batch_size):\n",
    "    # Create data loader\n",
    "    data = TensorDataset(torch.from_numpy(array).type(torch.LongTensor))\n",
    "    loader = DataLoader(data, batch_size=batch_size, drop_last=False)\n",
    "    return loader\n",
    "\n",
    "x1 = create_data_loader(x1)\n",
    "x2 = create_data_loader(x2)\n",
    "\n",
    "pred_list = np.array([])\n",
    "y_true = np.array([])\n",
    "y_pred = np.array([])\n",
    "# att1_list = []\n",
    "# att2_list = []\n",
    "for a, b in tqdm(zip(x1, x2)):\n",
    "    # Send data to graphic card - Cuda0\n",
    "    a, b = a[0].to(device), b[0].to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = model(a, b)\n",
    "        pred = pred.cpu()\n",
    "#         att1 = att1.cpu()\n",
    "#         att2 = att2.cpu()\n",
    "        y_true_curr = np.zeros(len(pred))\n",
    "        y_true_curr[test2_n:] = 1\n",
    "        y_true = np.concatenate([y_true, y_true_curr])\n",
    "\n",
    "        y_pred_curr = np.ones(len(pred))\n",
    "        y_pred_curr[np.where(pred <= 0)[0]] = 0\n",
    "        y_pred = np.concatenate([y_pred, y_pred_curr])\n",
    "        \n",
    "        pred_list = np.concatenate([pred_list, pred.squeeze().data.numpy()])\n",
    "#         att1_list.append(att1)\n",
    "#         att2_list.append(att2)\n",
    "\n",
    "print('Accuracy:\\t{}\\t\\tF1-score:\\t{}\\t\\t'.format(\n",
    "    round(accuracy_score(y_true, y_pred), 4),\n",
    "    round(f1_score(y_true, y_pred), 4)), end='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test case for stacked embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "batch = torch.zeros([0, emb_dim])\n",
    "batch_size = 500\n",
    "current_size = 0\n",
    "best_lost = None\n",
    "epochs = 1\n",
    "\n",
    "model.train()\n",
    "\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "start_time = time.time()\n",
    "avg_loss = 0\n",
    "avg_acc = 0\n",
    "count = 0\n",
    "\n",
    "anc_x = torch.zeros([0, emb_dim])\n",
    "pos_x = torch.zeros([0, emb_dim])\n",
    "neg_x = torch.zeros([0, emb_dim])\n",
    "for index, row in df_triplet_orders.iterrows():\n",
    "    anc_loc, pos_loc, neg_loc = row[['anchor', 'pos', 'neg']]\n",
    "\n",
    "    anc = df.loc[anc_loc, 'content'][0]\n",
    "    pos = df.loc[pos_loc, 'content'][0]\n",
    "    neg = df.loc[neg_loc, 'content'][0]\n",
    "    anc = stack_embedding(anc, document_embeddings)\n",
    "    pos = stack_embedding(pos, document_embeddings)\n",
    "    neg = stack_embedding(neg, document_embeddings)\n",
    "\n",
    "    if current_size < batch_size:\n",
    "        # Create DataLoader with batchsize is 500\n",
    "        anc_x = torch.cat((anc_x, anc.cpu()), 0)\n",
    "        pos_x = torch.cat((pos_x, pos.cpu()), 0)\n",
    "        neg_x = torch.cat((neg_x, neg.cpu()), 0)\n",
    "        current_size += 1\n",
    "    else:\n",
    "        # Training model per batch\n",
    "        anc_x, pos_x, neg_x = anc_x.to(device), pos_x.to(device), neg_x.to(device)\n",
    "        pos_pred, neg_pred = model(anc_x, pos_x, neg_x)\n",
    "\n",
    "        loss = (pos_pred + neg_pred).mean()\n",
    "        corrects = torch.sum(pos_pred == 0) + torch.sum(neg_pred == 0)\n",
    "        accuracy = float(corrects) / (2 * len(anc_x))\n",
    "        avg_acc += accuracy\n",
    "        avg_loss += float(loss)\n",
    "        # Gradient\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()  # Empty cuda cache\n",
    "        print('\\rBatch:\\t{}\\t\\tLoss:\\t{}\\t\\tAccuracy:\\t{}\\t\\t'.format(count, round(float(loss), 4),\n",
    "                                                                  round(accuracy, 4)), end='')\n",
    "\n",
    "        # Reset data loader and increase count\n",
    "        current_size = 1\n",
    "        anc_x = anc.cpu()\n",
    "        pos_x = pos.cpu()\n",
    "        neg_x = neg.cpu()\n",
    "        count += 1\n",
    "\n",
    "# Average loss and accuracy\n",
    "avg_acc = avg_acc / (len(df)/batch_size)\n",
    "avg_loss = avg_loss / (len(df)/batch_size)\n",
    "loss_list.append(avg_loss)\n",
    "acc_list.append(avg_acc)\n",
    "print('\\rAverage Loss:\\t{}\\t\\tAvg Accuracy:\\t{}\\t\\t'.format(round(avg_loss, 4),\n",
    "                                                                        round(avg_acc, 4)))\n",
    "        \n",
    "print(\"--- %s seconds ---\"%(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with another dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c197daee1657402d8dfeb45ec462571d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Padding', max=132, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32baf59fdd91497dbdd88137d3ebd5b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8646), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fd_df = pd.read_csv(path + 'fd_content.csv').iloc[:132, :]\n",
    "fd_df.fillna('', inplace=True)\n",
    "\n",
    "# test_df = shuffle(test_df)\n",
    "fd_df['content'] = fd_df['name'].str.lower() + ' ' + fd_df['address'].str.lower()\n",
    "fd_df['content'] = fd_df['content'].str.replace('\\n', ' ').str.replace(',' ,' ').str.replace(r'[ ]+', ' ', regex=True)\n",
    "\n",
    "fd_df = fd_df.loc[:, ['Unnamed: 0', 'content']]\n",
    "fd_df['cid'] = fd_df.loc[:, 'Unnamed: 0']\n",
    "del(fd_df['Unnamed: 0'])\n",
    "\n",
    "# Padding fd_df\n",
    "fd_arr = load_padded_data(pd.DataFrame(fd_df), word_to_index, dump_path=None,\n",
    "                                   pad_size=pad_size, retrain=True)\n",
    "\n",
    "# Split data set to anchor and object and y_true for testing model\n",
    "fd_anchor_arr = []\n",
    "fd_object_arr = []\n",
    "y = []\n",
    "for i, j in tqdm(list(itertools.combinations(range(0, len(fd_arr)), 2))):\n",
    "    # Loop through combination of fd dataset\n",
    "    fd_anchor_arr.append(fd_arr[i])\n",
    "    fd_object_arr.append(fd_arr[j])\n",
    "    # Get true label by cid (column 1)\n",
    "    y.append(1 if fd_df.iloc[i, 1]==fd_df.iloc[j, 1] else 0)\n",
    "    \n",
    "fd_anchor_arr, fd_object_arr, y = np.array(fd_anchor_arr), np.array(fd_object_arr), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75de07fbc1940d88f9f111460a4a19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:\t0.2451\t\tF1-score:\t0.0015\t\t"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data_utils\n",
    "\n",
    "fd_anchor = data_utils.TensorDataset(torch.from_numpy(fd_anchor_arr).type(torch.LongTensor))\n",
    "fd_anchor = data_utils.DataLoader(fd_anchor, batch_size=batch_size, drop_last=False)\n",
    "\n",
    "fd_object = data_utils.TensorDataset(torch.from_numpy(fd_object_arr).type(torch.LongTensor))\n",
    "fd_object = data_utils.DataLoader(fd_object, batch_size=batch_size, drop_last=False)\n",
    "\n",
    "y_true = data_utils.TensorDataset(torch.from_numpy(y).type(torch.DoubleTensor))\n",
    "y_true = data_utils.DataLoader(y, batch_size=batch_size, drop_last=False)\n",
    "\n",
    "\n",
    "fd_pred_list = np.array([])\n",
    "fd_y_pred = np.array([])\n",
    "fd_y_true = np.array([])\n",
    "for anc, obj, y_t in tqdm(zip(fd_anchor, fd_object, y_true)):\n",
    "    # Predict for each batch\n",
    "    anc, obj, y_t  = anc[0].to(device), obj[0].to(device), y_t\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = model(anc, obj)\n",
    "        pred = pred.cpu()\n",
    "\n",
    "        y_pred_curr = np.ones(len(pred))\n",
    "        y_pred_curr[np.where(pred <= 0)[0]] = 0\n",
    "        fd_y_pred = np.concatenate([fd_y_pred, y_pred_curr])\n",
    "        \n",
    "        fd_pred_list = np.concatenate([fd_pred_list, pred.squeeze().data.numpy()])\n",
    "        fd_y_true = np.concatenate([fd_y_true, y_t])\n",
    "\n",
    "print('Accuracy:\\t{}\\t\\tF1-score:\\t{}\\t\\t'.format(\n",
    "    round(accuracy_score(fd_y_true, fd_y_pred), 4),\n",
    "    round(f1_score(fd_y_true, fd_y_pred), 4)), end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9580,  0.8658,  0.7956, -0.7032, -0.7825, -0.9492,  0.9857,  0.2551,\n",
      "        -0.8189, -0.9438, -0.9957, -0.2437,  0.7037, -0.8359, -0.9853, -0.0531,\n",
      "        -0.8023, -0.9999, -0.4756, -0.9884, -0.9199,  0.3313,  0.8672, -0.6696,\n",
      "        -0.9626,  0.9931, -0.9924, -0.9847,  0.8892,  0.9809, -0.9619, -0.3206,\n",
      "         0.9818, -1.0000, -0.9550,  0.8916, -0.9987, -0.6110, -0.2472, -0.9990,\n",
      "        -0.9927,  0.9552, -0.1526, -1.0000,  0.3961, -0.9918,  0.9762,  0.9557,\n",
      "        -0.9377,  0.8395,  0.9892, -0.9999, -0.7036,  0.5103, -0.9135, -0.6444,\n",
      "        -0.9998, -0.9998,  0.5157, -0.5695, -0.8669, -0.7765, -0.4961, -0.9981,\n",
      "         0.3463, -0.6963, -0.9613, -0.9985, -0.2059, -1.0000,  0.5956, -0.9695,\n",
      "        -0.9947, -0.9957, -0.2103, -0.9456, -1.0000,  0.9994, -0.8633,  0.9705,\n",
      "         0.9993, -0.9973,  0.9895, -0.5695, -0.9974,  0.9278, -0.9167, -0.9941,\n",
      "        -0.9915, -0.9871, -0.9487,  0.4247,  0.9987, -0.9971, -0.9999, -0.9993,\n",
      "        -0.9845, -0.3585, -0.9939,  0.3712])\n"
     ]
    }
   ],
   "source": [
    "print(pred.view(-1)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd_y_true[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0, 1024,    0,    0, 1568,    0, 1494,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0, 1024,    0,    0, 1568,    0, 1494,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0, 1024,    0,    0, 1568,    0, 1494,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0, 1024,    0,    0, 1568,    0, 1494,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0, 1024,    0,    0, 1568,    0, 1494,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd_anchor_arr[fd_y_true != fd_y_pred][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0, 1416,    0,  630,    0,    0,    0,    0, 1416,    0,\n",
       "         630,    0,    0,    0, 1774,    0,    0,    0,    0],\n",
       "       [   0,  502,    0,    0, 1494,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,  984,    0,    0,    0,  980,    0,  493,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,  911,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [ 465, 1251,    0,    0, 1621,    0, 1251,    0,  985,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd_object_arr[fd_y_true != fd_y_pred][:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
